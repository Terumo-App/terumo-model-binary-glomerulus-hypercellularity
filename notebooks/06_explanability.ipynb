{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "# from src.dataset import ImageDataLoader\n",
    "# from src import config \n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from src.model import Net\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from grad_cam import GradCam,GuidedBackpropReLUModel,show_cams,show_gbs,preprocess_image\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "hiper_path = '../artifacts/hiper_others_2023-05-12-18-06-48/4_fold_max_acc_checkpoint.pth.tar'\n",
    "membran_path = '../artifacts/membran_others_2023-05-12-20-31-18/0_fold_max_acc_checkpoint.pth.tar'\n",
    "sclero_path = '../artifacts/sclero_others_2023-05-12-13-27-54/4_fold_max_acc_checkpoint.pth.tar'\n",
    "normal_path = '../artifacts/normal_others_2023-05-12-15-45-47/4_fold_max_acc_checkpoint.pth.tar'\n",
    "\n",
    "\n",
    "\n",
    "hiper_model = load_model_weights(Net(net_version=\"b0\", num_classes=2).to(DEVICE), hiper_path)\n",
    "membran_model = load_model_weights(Net(net_version=\"b0\", num_classes=2).to(DEVICE), membran_path)\n",
    "sclero_model = load_model_weights(Net(net_version=\"b0\", num_classes=2).to(DEVICE), sclero_path)\n",
    "normal_model = load_model_weights(Net(net_version=\"b0\", num_classes=2).to(DEVICE), normal_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dStaticSamePadding(\n",
       "  320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "  (static_padding): Identity()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiper_model.backbone._conv_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\terumo-model-binary-glomerulus-hypercellularity\\notebooks\\06_explanability.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m targets \u001b[39m=\u001b[39m [ClassifierOutputTarget(\u001b[39m281\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m grayscale_cam \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49minput_tensor, targets\u001b[39m=\u001b[39;49mtargets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X24sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# In this example grayscale_cam has only one image in the batch:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X24sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m grayscale_cam \u001b[39m=\u001b[39m grayscale_cam[\u001b[39m0\u001b[39m, :]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_grad_cam\\base_cam.py:188\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[1;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[0;32m    186\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[1;32m--> 188\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[0;32m    189\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_grad_cam\\base_cam.py:74\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[1;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_input_gradient:\n\u001b[0;32m     71\u001b[0m     input_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mVariable(input_tensor,\n\u001b[0;32m     72\u001b[0m                                            requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivations_and_grads(input_tensor)\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     target_categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pytorch_grad_cam\\activations_and_gradients.py:42\u001b[0m, in \u001b[0;36mActivationsAndGradients.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradients \u001b[39m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\terumo-model-binary-glomerulus-hypercellularity\\notebooks\\..\\src\\model.py:13\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\efficientnet_pytorch\\model.py:314\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39m\"\"\"EfficientNet's forward function.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39m    Output of this model after processing.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m# Convolution layers\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(inputs)\n\u001b[0;32m    315\u001b[0m \u001b[39m# Pooling and final linear layer\u001b[39;00m\n\u001b[0;32m    316\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_avg_pooling(x)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\efficientnet_pytorch\\model.py:289\u001b[0m, in \u001b[0;36mEfficientNet.extract_features\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39m\"\"\"use convolution layer to extract feature .\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \n\u001b[0;32m    281\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39m    layer in the efficientnet model.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# Stem\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swish(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bn0(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_stem(inputs)))\n\u001b[0;32m    291\u001b[0m \u001b[39m# Blocks\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39mfor\u001b[39;00m idx, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocks):\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m--> 410\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n",
      "\u001b[1;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "model = hiper_model\n",
    "target_layers = [model.backbone._conv_head]\n",
    "img = cv2.imread('../data/raw/hipercellularity/PAS/PSHIPERCELULARIDADE20200802-1394.jpg', 1)\n",
    "img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "input_tensor = torch.from_numpy(img)\n",
    "input_tensor = input_tensor.reshape(3, 224, 224)\n",
    "# input_tensor = # Create an input tensor image for your model..\n",
    "# Note: input_tensor can be a batch tensor with several images!\n",
    "\n",
    "# Construct the CAM object once, and then re-use it on many images:\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "# You can also use it within a with statement, to make sure it is freed,\n",
    "# In case you need to re-create it inside an outer loop:\n",
    "# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n",
    "#   ...\n",
    "\n",
    "# We have to specify the target we want to generate\n",
    "# the Class Activation Maps for.\n",
    "# If targets is None, the highest scoring category\n",
    "# will be used for every image in the batch.\n",
    "# Here we use ClassifierOutputTarget, but you can define your own custom targets\n",
    "# That are, for example, combinations of categories, or specific outputs in a non standard model.\n",
    "\n",
    "targets = [ClassifierOutputTarget(281)]\n",
    "\n",
    "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "# In this example grayscale_cam has only one image in the batch:\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "visualization = show_cam_on_image(input_tensor, grayscale_cam, use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .backbone._blocks[1]\n",
    "# hiper_model.backbone._modules.items()\n",
    "hiper_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "[tensor([[[[-1.5374e+01, -1.6416e+01,  3.6840e+00,  ...,  9.9752e+00,\n",
      "            2.5049e+01,  1.2364e+01],\n",
      "          [ 7.2272e-01,  5.2604e-01, -6.2175e+00,  ...,  6.9073e+00,\n",
      "            4.2774e+00, -6.1703e+00],\n",
      "          [ 1.2355e+01, -7.5784e+00, -1.2004e+01,  ..., -1.3154e+01,\n",
      "            1.5170e+01,  2.6351e+01],\n",
      "          ...,\n",
      "          [-3.7182e+00,  2.2605e+00, -2.2916e-01,  ..., -7.7778e+00,\n",
      "            1.6496e+00,  1.8565e+00],\n",
      "          [ 5.3102e-02, -1.2258e+00, -2.3844e+00,  ..., -4.3670e+00,\n",
      "            8.3668e-01,  1.5869e+00],\n",
      "          [-3.6608e+01, -4.1236e+01, -3.9795e+01,  ..., -4.9284e+01,\n",
      "           -5.2839e+01, -3.2216e+01]],\n",
      "\n",
      "         [[ 2.8277e+01,  2.5345e+01,  2.9307e+01,  ..., -9.5858e-01,\n",
      "           -8.6768e+00, -1.1813e+01],\n",
      "          [ 1.5263e+01,  6.1263e+00,  1.5278e+01,  ...,  4.7722e+00,\n",
      "           -2.1145e+00, -2.3680e+00],\n",
      "          [-1.0148e+01, -1.3872e+00, -2.9319e+00,  ...,  6.3977e+00,\n",
      "           -7.2324e+00,  2.0192e+01],\n",
      "          ...,\n",
      "          [ 1.8727e+01,  1.2392e+01,  1.7351e+01,  ...,  2.8402e+01,\n",
      "            1.7810e+01,  1.6544e+01],\n",
      "          [ 1.3482e+01,  1.6928e+01,  1.4790e+01,  ...,  2.2874e+01,\n",
      "            1.9612e+01,  1.3903e+01],\n",
      "          [ 2.3305e+01,  2.6839e+01,  2.2695e+01,  ...,  2.8543e+01,\n",
      "            2.9311e+01,  7.6552e+00]],\n",
      "\n",
      "         [[ 9.8892e+00,  1.3892e+01, -3.8156e+00,  ..., -1.4907e+01,\n",
      "           -1.0652e+00,  1.6636e+00],\n",
      "          [ 1.0255e+01, -6.8761e+00,  6.0956e+00,  ...,  3.3582e+00,\n",
      "            1.8015e+00, -3.3914e+01],\n",
      "          [ 1.7854e+01,  2.4364e+00,  1.7036e+01,  ...,  6.7210e+00,\n",
      "           -1.2510e+01, -4.5977e+01],\n",
      "          ...,\n",
      "          [ 1.2933e+00,  1.0395e+01,  7.1556e+00,  ..., -4.3883e+00,\n",
      "            4.4877e+00,  7.7090e+00],\n",
      "          [ 1.9686e+00,  4.0998e+00,  4.9370e+00,  ...,  3.6742e+00,\n",
      "            1.0454e+01,  6.6831e+00],\n",
      "          [ 2.3037e+01,  2.3295e+01,  2.5302e+01,  ...,  2.3946e+01,\n",
      "            2.5076e+01,  3.0882e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.2087e+01,  8.4897e+01,  9.1781e+01,  ...,  2.8517e+01,\n",
      "            1.5378e+01,  9.5359e+00],\n",
      "          [ 6.5362e+01,  5.4960e+01,  5.7740e+01,  ..., -1.3986e+01,\n",
      "            2.3198e+01,  5.2145e+00],\n",
      "          [ 7.7180e+01,  5.9292e+01,  6.7609e+01,  ...,  3.1615e+00,\n",
      "            2.4162e+01,  3.8110e+01],\n",
      "          ...,\n",
      "          [ 5.6752e+01,  4.9150e+01,  4.3065e+01,  ...,  6.9671e+01,\n",
      "            5.8123e+01,  6.3820e+01],\n",
      "          [ 4.4743e+01,  4.2307e+01,  4.1507e+01,  ...,  6.3642e+01,\n",
      "            5.9601e+01,  6.3899e+01],\n",
      "          [ 1.8986e+01,  1.6314e+01,  2.0843e+01,  ...,  3.4069e+01,\n",
      "            3.2275e+01,  5.0500e+01]],\n",
      "\n",
      "         [[ 2.3261e+01,  2.0247e+01,  2.0894e+01,  ..., -7.4767e+00,\n",
      "           -7.2664e+00, -1.4465e+01],\n",
      "          [ 3.1140e+01,  2.0388e+01,  2.5706e+01,  ..., -1.6177e+00,\n",
      "           -1.1130e+01, -1.3464e+01],\n",
      "          [ 2.6013e+01, -1.0747e+01,  2.1576e-01,  ..., -1.9237e+01,\n",
      "           -5.3840e+00, -1.5253e+01],\n",
      "          ...,\n",
      "          [ 2.2017e+01,  2.3327e+01,  2.2222e+01,  ...,  2.5533e+01,\n",
      "            2.8768e+01,  2.1108e+01],\n",
      "          [ 2.3066e+01,  2.4908e+01,  2.0717e+01,  ...,  2.6893e+01,\n",
      "            2.8504e+01,  2.2815e+01],\n",
      "          [ 1.4736e+00,  3.0884e+00,  4.2361e+00,  ...,  7.3178e+00,\n",
      "            3.9406e+00, -1.4141e+00]],\n",
      "\n",
      "         [[-2.8693e+01, -2.2873e+01, -2.3240e+01,  ..., -2.1580e+01,\n",
      "           -1.2188e+01, -1.1031e+01],\n",
      "          [-3.1638e+01, -2.5662e+01, -2.1218e+01,  ..., -2.1559e+01,\n",
      "            2.2457e+00,  8.0197e+00],\n",
      "          [-3.2474e+01, -1.2979e+01, -2.3036e+01,  ..., -7.9003e+00,\n",
      "           -1.2073e+01,  1.5750e+01],\n",
      "          ...,\n",
      "          [-3.2285e+01, -2.7678e+01, -2.5885e+01,  ..., -2.1834e+01,\n",
      "           -2.7368e+01, -2.3118e+01],\n",
      "          [-2.9018e+01, -2.5277e+01, -2.4119e+01,  ..., -2.2516e+01,\n",
      "           -2.7162e+01, -2.3520e+01],\n",
      "          [-9.7202e+00,  1.9395e+00,  1.2422e+00,  ...,  4.1563e-01,\n",
      "            4.6804e+00,  1.0155e+01]]]], grad_fn=<NativeBatchNormBackward0>), tensor([[[[ 6.2962e+00, -1.6572e+01,  2.0542e+01,  ..., -2.4027e+00,\n",
      "            1.5040e+01,  2.5110e+00],\n",
      "          [ 1.9600e+01,  1.1499e+01, -9.4711e+00,  ..., -6.0998e+00,\n",
      "           -4.4416e+00, -4.9306e+00],\n",
      "          [ 2.7397e+01, -2.1343e-01, -1.9958e+01,  ..., -2.3350e+01,\n",
      "            6.6202e+00,  3.0838e+01],\n",
      "          ...,\n",
      "          [ 8.7368e-01,  4.1180e+00,  6.6536e+00,  ..., -1.3326e+01,\n",
      "            1.7044e+01,  2.1437e+01],\n",
      "          [ 6.1380e-01, -8.9137e+00, -4.0745e+00,  ..., -9.9025e+00,\n",
      "           -2.9716e+00,  2.5392e+00],\n",
      "          [-1.3892e+01, -3.1874e+01, -2.4883e+01,  ..., -3.1707e+01,\n",
      "           -3.8309e+01, -8.6325e+00]],\n",
      "\n",
      "         [[ 1.4714e+01,  4.3582e+01,  3.7657e+01,  ...,  4.0414e+00,\n",
      "           -3.5757e+00, -1.4624e+00],\n",
      "          [-7.6206e+00,  3.4577e+00,  1.4130e+01,  ...,  1.3919e+01,\n",
      "            1.8197e+00,  2.1160e+00],\n",
      "          [-3.5208e+01, -8.8055e+00,  2.5521e+00,  ...,  1.5601e+01,\n",
      "           -2.5428e+00,  1.3287e+01],\n",
      "          ...,\n",
      "          [ 1.9910e+01,  1.1559e+01,  1.6466e+01,  ...,  2.9663e+01,\n",
      "           -6.6829e-01,  1.3504e-02],\n",
      "          [-9.0877e+00,  9.5416e+00, -6.1618e-02,  ...,  9.0458e+00,\n",
      "            4.7897e+00,  9.9292e-01],\n",
      "          [-1.4297e+00,  1.9418e+01,  1.3960e+01,  ...,  1.3501e+01,\n",
      "            1.4020e+01, -2.3419e+01]],\n",
      "\n",
      "         [[-1.5057e+01,  2.7202e+01,  4.2735e+00,  ...,  4.6474e+00,\n",
      "            1.1286e+01,  1.5887e+01],\n",
      "          [-6.1238e+00,  1.1870e+01,  2.9710e+01,  ...,  2.7197e+01,\n",
      "            9.3974e+00, -8.3205e+00],\n",
      "          [-1.2411e+01,  1.1340e+01,  2.3294e+01,  ...,  2.4979e+01,\n",
      "            3.6171e+00, -3.1366e+01],\n",
      "          ...,\n",
      "          [-2.2157e+00,  3.0065e+01,  1.7224e+01,  ...,  3.2327e+00,\n",
      "            1.1275e+01, -9.1926e+00],\n",
      "          [-9.2372e+00,  1.9300e+01,  1.4632e+01,  ...,  1.1770e+01,\n",
      "            2.6833e+01, -1.2268e+00],\n",
      "          [-1.1883e+01,  2.0426e+01,  2.4990e+01,  ...,  1.9886e+01,\n",
      "            2.0013e+01, -4.8740e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.7244e+01,  6.3125e+01,  8.3866e+01,  ...,  1.8570e+01,\n",
      "            5.2949e+00, -2.7454e+00],\n",
      "          [ 1.9321e+01,  4.0007e+01,  4.2777e+01,  ..., -2.6733e+01,\n",
      "            1.2598e+01, -6.0867e+00],\n",
      "          [ 5.4192e+01,  4.5099e+01,  4.5792e+01,  ..., -9.4081e+00,\n",
      "            2.4247e+01,  3.4081e+01],\n",
      "          ...,\n",
      "          [ 4.6137e+01,  5.4486e+01,  4.3300e+01,  ...,  7.4150e+01,\n",
      "            5.3136e+01,  4.2417e+01],\n",
      "          [ 1.8652e+01,  4.0394e+01,  3.6540e+01,  ...,  5.9300e+01,\n",
      "            5.8277e+01,  5.4938e+01],\n",
      "          [ 2.6145e+00,  4.8710e+00,  1.2025e+01,  ...,  2.6939e+01,\n",
      "            2.9117e+01,  5.8513e+01]],\n",
      "\n",
      "         [[ 3.3724e+01,  6.6740e+00,  3.5045e+01,  ..., -7.4220e+00,\n",
      "           -1.0435e-01,  5.9813e-01],\n",
      "          [ 1.4205e+01,  2.6164e+01,  1.8034e+01,  ..., -3.5484e+01,\n",
      "           -1.3606e+01, -1.2945e+01],\n",
      "          [ 4.1153e+01,  1.7347e+01, -3.3735e+00,  ..., -1.9149e+01,\n",
      "            2.8641e+00, -6.5195e+00],\n",
      "          ...,\n",
      "          [ 3.5734e+01,  2.8318e+01,  5.1177e+00,  ...,  1.1874e+01,\n",
      "            2.3162e+01,  8.1472e+00],\n",
      "          [ 2.1853e+01,  1.3447e+01,  2.8986e+01,  ...,  2.1328e+01,\n",
      "            2.8196e+01,  2.1544e+01],\n",
      "          [ 1.9663e+01, -2.5714e+01, -1.9203e+01,  ..., -3.2081e+01,\n",
      "           -1.9789e+01,  6.0888e+00]],\n",
      "\n",
      "         [[-2.6624e+01, -5.8014e+00, -1.4235e+01,  ..., -1.8285e+01,\n",
      "           -6.7105e+00, -6.2256e-02],\n",
      "          [-1.6870e+01, -5.4715e+00,  6.9361e+00,  ..., -1.1133e+01,\n",
      "            9.6683e+00,  1.2865e+01],\n",
      "          [-1.7592e+01,  4.2448e+00, -1.0139e+01,  ...,  8.3699e-01,\n",
      "           -6.0779e+00,  2.1638e+01],\n",
      "          ...,\n",
      "          [-2.4459e+01, -1.6193e+01, -1.3505e+01,  ..., -1.2666e+01,\n",
      "           -2.3891e+01, -1.0334e+01],\n",
      "          [-1.4270e+01, -9.3023e+00, -6.2424e+00,  ..., -2.8636e+00,\n",
      "           -8.0645e+00, -3.4145e+00],\n",
      "          [-1.1624e+01, -2.4261e+00, -2.7304e+00,  ..., -6.2697e+00,\n",
      "           -2.8344e+00,  9.7022e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ -6.5645, -12.3247,   8.2118,  ...,  26.9837,  20.4424,  20.7051],\n",
      "          [ 17.0411,  -0.0644,  10.6499,  ...,  -0.4813,   7.0390,  20.4628],\n",
      "          [ 28.8384,  10.0809,  10.1042,  ...,  -3.3795,  19.6715,  13.0744],\n",
      "          ...,\n",
      "          [ 27.3650,  29.2868,   9.7929,  ..., -41.5533, -13.4224, -14.6374],\n",
      "          [  6.9261, -20.3758, -17.4486,  ..., -21.5786, -30.1663, -12.7945],\n",
      "          [ 28.8203,  14.2632,  20.1133,  ...,  14.5792,  14.6309,  13.7058]],\n",
      "\n",
      "         [[  3.7677, -23.5981, -39.2705,  ...,   1.2868,  -2.6386, -18.9485],\n",
      "          [-14.3890, -29.9127, -27.4715,  ...,   5.8159, -14.1400, -25.8111],\n",
      "          [ -7.5593,  -4.7644,   3.0007,  ..., -32.9025, -39.8861, -40.1831],\n",
      "          ...,\n",
      "          [-47.4444, -13.8174, -32.0405,  ...,  27.3292,  36.8940, -33.2679],\n",
      "          [-22.1699,  22.1204,  19.7906,  ...,  30.7146,  33.7960, -25.1868],\n",
      "          [  2.9546,  17.5278,  19.1045,  ...,  18.6950,  23.8394, -23.2946]],\n",
      "\n",
      "         [[  7.0024,  19.5366,  10.3930,  ...,   7.6369,   5.4844,  11.4669],\n",
      "          [ 25.2457,  14.1931, -11.8880,  ...,  29.6511,   3.5332,  10.2360],\n",
      "          [  1.5721,  18.0243,  -8.6542,  ...,   0.8622,  21.9717,   4.7797],\n",
      "          ...,\n",
      "          [  7.9746,  18.7646,  14.4345,  ...,  -5.7546,  -2.4746,   8.0160],\n",
      "          [ 21.3984,  20.5059,  28.7803,  ...,   8.1343,  -1.7653,   4.1250],\n",
      "          [-18.4702, -16.0918, -14.8963,  ..., -15.4585, -17.7671, -15.5321]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-24.8721, -19.8296,  -4.9486,  ...,  -0.6746,  -0.0920,   2.1525],\n",
      "          [-18.4336,  -7.4028, -11.9275,  ...,  -7.7262,   0.9288, -10.1710],\n",
      "          [ -8.4869, -13.5023,  -6.7626,  ..., -29.1981,   8.0710,   7.1581],\n",
      "          ...,\n",
      "          [ 11.9022,   0.8529,   5.1142,  ..., -26.4873, -29.2652,  -5.6150],\n",
      "          [-15.7878, -11.6145, -10.3713,  ..., -39.6989, -39.4439, -11.3426],\n",
      "          [  3.3591,  -0.4415,  -2.9428,  ...,  -7.5585,  -4.5406,   7.8854]],\n",
      "\n",
      "         [[-11.1773, -15.1345, -11.2948,  ...,  -5.2074, -11.5772, -11.7876],\n",
      "          [  5.8492,   9.3837,   0.7002,  ..., -14.4213,   2.7593,  -7.7536],\n",
      "          [ -5.0799,   1.0562, -11.5158,  ...,   5.5854,  15.0822,  -0.6647],\n",
      "          ...,\n",
      "          [  0.1962,  19.8423,   0.3970,  ..., -15.1925, -31.0119, -32.4303],\n",
      "          [-15.7118, -14.9585,  -6.5829,  ..., -11.2798, -12.7533, -30.4632],\n",
      "          [-25.4780, -28.6515, -26.1316,  ..., -28.8979, -27.1880, -27.3801]],\n",
      "\n",
      "         [[ 37.2041,   0.3864,  -3.9582,  ...,   1.3716,  -2.8598, -10.9399],\n",
      "          [ 20.3019,  -0.1996,  -3.2059,  ...,  -5.2651,  -0.4246, -12.3372],\n",
      "          [  2.0873,  -3.0649,  -4.1622,  ...,  -2.3320,   9.8733, -11.2507],\n",
      "          ...,\n",
      "          [  5.6259,  12.8727,   5.9791,  ...,  14.5550,  14.8816,   4.6604],\n",
      "          [ 12.8860,  20.7723,  18.4213,  ...,  20.4029,  19.1037,   4.2448],\n",
      "          [ 46.9243,  52.8429,  52.9055,  ...,  58.1989,  63.5824,  25.1449]]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>), tensor([[[[ -5.2419, -12.9541,   4.8101,  ...,  22.0423,  20.6771,  19.5162],\n",
      "          [ 17.6890,  -3.5384,  10.4517,  ...,  -4.0595,   9.0807,  20.0513],\n",
      "          [ 30.3348,   8.2901,   8.4134,  ...,  -1.0080,  18.5897,  13.1379],\n",
      "          ...,\n",
      "          [ 23.3113,  29.1646,   6.4524,  ..., -47.7869,  -8.7929, -14.7331],\n",
      "          [  5.6640, -21.0886, -21.2353,  ..., -19.6893, -35.2705, -11.8667],\n",
      "          [ 24.0689,   7.3137,  13.1421,  ...,   7.0654,  10.2540,   9.9532]],\n",
      "\n",
      "         [[  3.3855, -19.3434, -52.7543,  ...,   3.6308,  -3.3468, -19.6976],\n",
      "          [-30.3425, -42.2060, -29.9293,  ...,   6.9840, -24.2828, -21.7474],\n",
      "          [ -7.8496,  -1.1285,   7.0699,  ..., -27.8016, -43.9293, -33.7702],\n",
      "          ...,\n",
      "          [-58.5377, -15.4467, -44.4629,  ...,  10.9271,  21.5275, -35.7278],\n",
      "          [-32.8227,  11.6473,   7.9755,  ...,  17.8658,  20.2697, -32.6511],\n",
      "          [-31.9698,  -6.8856,  -4.7716,  ..., -14.6340, -11.4373, -49.4503]],\n",
      "\n",
      "         [[  8.4452,  20.2351,  13.9428,  ...,   9.2336,   9.6066,  16.6146],\n",
      "          [ 22.5635,  14.2811,  -9.9894,  ...,  32.9163,  12.5709,   8.4751],\n",
      "          [  4.2428,  18.9067,  -5.6934,  ...,  -1.7862,  26.5890,   6.4969],\n",
      "          ...,\n",
      "          [  9.2522,  19.6847,  12.5444,  ...,  -6.4998,   2.0614,   8.1865],\n",
      "          [ 21.4253,  23.9213,  36.2133,  ...,  11.4058,  -1.2387,   6.6989],\n",
      "          [-21.2243, -11.6582,  -8.2106,  ...,  -9.3247, -11.8080, -13.1080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -2.5488,  -5.2734,  -1.7137,  ...,   0.3716,  -0.2975,  -1.2399],\n",
      "          [-10.3493,   1.1721,  -5.5999,  ...,  -5.2611,   6.1863, -13.4633],\n",
      "          [ -9.7954, -15.1786,  -5.5772,  ..., -19.8410,  13.1301,   1.4881],\n",
      "          ...,\n",
      "          [  6.8604,  10.6439,   7.4339,  ..., -13.7694, -17.2022,  -9.2826],\n",
      "          [-15.5489,  -5.6152,  -9.4867,  ..., -28.4339, -28.8314, -13.6370],\n",
      "          [  7.5793,   6.8510,  -0.6412,  ...,  -6.5943,   0.2585,   5.8586]],\n",
      "\n",
      "         [[-24.2062, -18.7474, -10.5542,  ..., -13.6198, -13.2402, -15.8110],\n",
      "          [ -2.9873,  10.0020,  -7.6852,  ..., -16.3425,  -5.4622, -13.7521],\n",
      "          [ -7.2670,  -1.1930, -13.6841,  ...,  -1.8990,  13.6905,   2.9305],\n",
      "          ...,\n",
      "          [  9.5609,  14.9905,  -2.0403,  ..., -33.2575, -47.8091, -34.5871],\n",
      "          [-26.5790, -28.2742, -17.5593,  ..., -23.0886, -28.6582, -31.3320],\n",
      "          [-40.3550, -42.7564, -34.2862,  ..., -37.9833, -42.7787, -29.9945]],\n",
      "\n",
      "         [[ 50.7306,  12.6557,  27.0713,  ...,  30.6497,  17.9433,  14.8051],\n",
      "          [ 60.8569,  35.6902,  -4.7947,  ...,  -2.8744,  17.5627,  -3.6438],\n",
      "          [ 14.5741,   1.9967,  -2.0704,  ...,  10.7669,  30.3481,  13.1805],\n",
      "          ...,\n",
      "          [ 51.3721,  40.9892,  40.4926,  ...,  47.1518,  31.7984,  32.1951],\n",
      "          [ 24.6233,  26.6961,  38.6774,  ...,  82.1346,  66.8618,  43.3720],\n",
      "          [ 95.4704, 112.3652, 116.8924,  ..., 147.1740, 139.7257,  84.9236]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[ 2.8285e+01,  2.1729e+01,  8.6094e+00,  ...,  1.6889e+00,\n",
      "            7.4601e+00,  9.6309e+00],\n",
      "          [ 1.6303e+01,  1.0719e+01,  1.3533e+00,  ...,  3.9263e+00,\n",
      "            7.9726e-01, -1.2798e+00],\n",
      "          [ 7.4467e+00,  7.7705e+00, -7.5562e+00,  ...,  1.6865e+01,\n",
      "            1.5711e+01,  3.6926e+00],\n",
      "          ...,\n",
      "          [ 9.6322e+00,  2.1968e+01,  1.5411e+01,  ...,  1.2010e+01,\n",
      "           -1.8086e+01,  9.3495e+00],\n",
      "          [ 6.8273e+00,  1.4430e+01,  1.1386e+01,  ..., -5.4034e+00,\n",
      "           -3.0607e+00,  1.0025e+00],\n",
      "          [ 3.1575e+01,  1.7518e+01,  2.0136e+01,  ...,  3.3801e+01,\n",
      "            2.8607e+01,  2.0120e+01]],\n",
      "\n",
      "         [[-2.4882e+01, -5.1118e+00, -5.9462e+00,  ..., -9.2657e+00,\n",
      "           -1.8270e+01, -9.8398e+00],\n",
      "          [-1.0786e+01,  1.7172e+00, -9.4405e+00,  ..., -2.6850e+00,\n",
      "            3.3248e+00, -8.3954e+00],\n",
      "          [ 1.0342e-01, -8.3476e+00, -1.4404e+01,  ..., -1.2166e+01,\n",
      "           -7.7776e+00, -1.2759e+01],\n",
      "          ...,\n",
      "          [-6.6540e+00, -3.3016e+00, -1.4659e+01,  ..., -6.6428e+00,\n",
      "           -1.4412e+01, -7.8206e+00],\n",
      "          [-2.6298e+01, -2.2059e+01, -1.3681e+01,  ..., -2.8497e+01,\n",
      "           -2.6822e+01,  2.3880e+00],\n",
      "          [-8.1993e+00,  1.4328e+00, -2.5834e+00,  ...,  2.9593e-01,\n",
      "           -8.2617e-01, -3.4640e+00]],\n",
      "\n",
      "         [[ 2.6546e+01,  1.0664e+01,  1.0665e+01,  ...,  8.1409e+00,\n",
      "            8.9726e+00, -5.2743e+00],\n",
      "          [ 1.1439e+01,  1.6285e+01,  7.4754e+00,  ...,  1.5388e+01,\n",
      "            2.6517e+01,  1.4458e+01],\n",
      "          [ 8.5718e+00,  1.2653e+01,  2.9475e+01,  ...,  1.6505e+01,\n",
      "            1.4800e+01,  6.9325e-01],\n",
      "          ...,\n",
      "          [ 2.0579e+01,  8.4863e+00,  1.6768e+01,  ...,  4.3978e+00,\n",
      "            1.9896e+01,  3.9313e+01],\n",
      "          [ 3.6573e+01,  2.7184e+01,  1.0106e+01,  ...,  6.1571e+01,\n",
      "            7.0045e+01,  3.1379e+01],\n",
      "          [ 2.1174e+01,  2.1871e+01,  9.4480e+00,  ...,  2.2846e+01,\n",
      "            2.0733e+01,  5.6308e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.5098e+00, -4.8687e+00, -4.0445e+00,  ..., -8.1095e+00,\n",
      "           -3.1863e+00,  5.7100e-01],\n",
      "          [ 2.2188e+00, -1.3422e-01, -3.3527e+00,  ...,  1.0320e+00,\n",
      "           -1.4000e+01,  5.5446e+00],\n",
      "          [-4.6803e+00, -6.5320e+00,  1.9374e+00,  ...,  1.0333e+00,\n",
      "           -8.8640e+00, -7.5159e-01],\n",
      "          ...,\n",
      "          [-3.6213e+00, -1.1091e+01, -7.0673e+00,  ..., -1.0838e+01,\n",
      "            4.7464e+00, -1.5990e+01],\n",
      "          [-9.5895e+00, -1.2281e+01, -4.3741e+00,  ..., -3.3407e+00,\n",
      "           -3.1628e+01, -2.6818e+01],\n",
      "          [-3.4907e+01, -2.1632e+01, -7.6687e+00,  ..., -3.0197e+01,\n",
      "           -3.5519e+01, -6.8271e+00]],\n",
      "\n",
      "         [[-4.6913e+00, -1.9354e+01, -5.9796e+00,  ..., -9.1149e+00,\n",
      "           -1.0357e+00,  1.2990e+00],\n",
      "          [-2.0924e+01, -1.3003e+01, -1.9491e+00,  ...,  5.6850e+00,\n",
      "           -1.4538e+01, -6.2265e+00],\n",
      "          [-8.4301e+00, -4.5764e+00, -1.4703e+00,  ...,  1.4705e+00,\n",
      "           -8.7600e+00,  3.0091e+00],\n",
      "          ...,\n",
      "          [-2.9094e+01, -1.2719e+01, -9.2811e+00,  ...,  1.7979e+00,\n",
      "           -1.3255e+00,  1.3427e+01],\n",
      "          [-7.2846e+00,  1.4095e+00,  3.1311e+00,  ...,  1.8145e+01,\n",
      "            1.7749e+01,  8.9388e+00],\n",
      "          [ 1.9247e+00,  2.9021e+00,  7.0484e+00,  ...,  6.7051e+00,\n",
      "            9.9704e-01, -6.5230e-02]],\n",
      "\n",
      "         [[-2.5886e+01, -3.1232e+01, -2.7573e+01,  ..., -2.2889e+01,\n",
      "           -2.2162e+01, -1.8589e+01],\n",
      "          [-2.4169e+01, -1.8280e+01, -3.3057e+01,  ..., -2.3166e+01,\n",
      "           -4.7202e+01, -2.5357e+01],\n",
      "          [-2.3752e+01, -2.2219e+01, -1.9768e+01,  ..., -2.0429e+01,\n",
      "           -2.1895e+01, -2.1262e+01],\n",
      "          ...,\n",
      "          [-2.9385e+01, -2.4697e+01, -2.0858e+01,  ..., -2.3756e+01,\n",
      "           -3.1903e+01, -1.3357e+01],\n",
      "          [-2.1785e+01, -1.6275e+01, -1.5428e+01,  ..., -3.5029e+01,\n",
      "           -2.0308e+01, -6.0314e+00],\n",
      "          [-1.4277e+00, -9.1327e+00,  2.3788e-01,  ..., -1.4109e+01,\n",
      "           -9.3016e+00, -1.3272e+01]]]], grad_fn=<NativeBatchNormBackward0>), tensor([[[[ 3.3118e+01,  2.2851e+01,  1.1776e+01,  ...,  6.5493e+00,\n",
      "            1.1044e+01,  1.0742e+01],\n",
      "          [ 2.0531e+01,  1.2403e+01,  1.0016e+01,  ...,  1.0931e+01,\n",
      "            5.3198e+00,  2.4470e-02],\n",
      "          [ 1.0515e+01,  1.3788e+01, -9.4171e-01,  ...,  2.2631e+01,\n",
      "            1.7261e+01,  5.8643e+00],\n",
      "          ...,\n",
      "          [ 1.1680e+01,  2.5147e+01,  1.6689e+01,  ...,  1.6457e+01,\n",
      "           -1.0727e+01,  1.3196e+01],\n",
      "          [ 9.3675e+00,  1.6602e+01,  1.5424e+01,  ...,  1.1031e+00,\n",
      "            4.2336e+00,  2.1930e+00],\n",
      "          [ 3.2945e+01,  1.6764e+01,  2.2283e+01,  ...,  3.0940e+01,\n",
      "            2.3891e+01,  1.3954e+01]],\n",
      "\n",
      "         [[-4.6301e+00, -1.0305e+00, -6.1951e-02,  ..., -5.9879e-01,\n",
      "            5.3867e-01,  1.0326e+01],\n",
      "          [ 3.8977e+00,  9.4584e+00,  7.7686e+00,  ..., -8.5298e-01,\n",
      "            1.2337e+01,  2.5593e+00],\n",
      "          [ 1.2552e+01, -1.0018e+00,  1.7633e+01,  ..., -4.1495e+00,\n",
      "           -3.2252e+00, -1.5444e+00],\n",
      "          ...,\n",
      "          [ 5.1923e+00,  4.5714e+00, -3.1271e-01,  ..., -8.5823e+00,\n",
      "            1.7144e+01,  2.7616e+01],\n",
      "          [-7.3614e+00, -5.4694e+00, -1.4660e+00,  ...,  6.7269e+00,\n",
      "            2.7993e+01,  3.4241e+01],\n",
      "          [ 4.7551e+00, -8.1156e+00, -6.9991e+00,  ...,  1.8334e+01,\n",
      "            1.0662e+01,  4.0010e+00]],\n",
      "\n",
      "         [[ 4.1170e+01,  1.6421e+01,  2.6198e+01,  ...,  2.4753e+01,\n",
      "            2.0375e+01,  5.4788e+00],\n",
      "          [ 1.7717e+01,  3.2936e+01,  2.6865e+01,  ...,  2.9407e+01,\n",
      "            3.4816e+01,  1.8722e+01],\n",
      "          [ 9.6652e+00,  2.8411e+01,  5.1010e+01,  ...,  2.7602e+01,\n",
      "            1.8649e+01,  3.1737e+00],\n",
      "          ...,\n",
      "          [ 2.6779e+01,  1.7375e+01,  2.3857e+01,  ...,  2.4020e+01,\n",
      "            3.7347e+01,  4.0813e+01],\n",
      "          [ 4.1395e+01,  3.3485e+01,  2.1132e+01,  ...,  6.9085e+01,\n",
      "            7.8053e+01,  4.4932e+01],\n",
      "          [ 5.6747e+00,  1.9707e+01,  9.8321e+00,  ...,  9.9227e+00,\n",
      "            1.8157e+01,  1.3450e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1129e+01, -2.3922e+01, -2.5780e+01,  ..., -2.0366e+01,\n",
      "           -1.1767e+01, -1.0838e+01],\n",
      "          [-7.2369e+00, -1.0879e+01, -2.2939e+01,  ..., -5.5776e+00,\n",
      "           -1.8143e+01, -7.4227e+00],\n",
      "          [-1.3545e+01, -1.7636e+01, -2.3334e+00,  ..., -1.5484e+01,\n",
      "           -1.2760e+01, -8.2756e+00],\n",
      "          ...,\n",
      "          [-1.5990e+01, -1.8656e+01, -1.5157e+01,  ..., -3.9318e+01,\n",
      "           -5.8965e+00, -3.5911e+01],\n",
      "          [-2.3674e+01, -1.6617e+01, -5.8962e+00,  ..., -2.7745e+01,\n",
      "           -4.3801e+01, -4.0930e+01],\n",
      "          [-4.0733e+01, -2.0036e+01, -7.8771e+00,  ..., -3.7803e+01,\n",
      "           -3.9932e+01, -1.4657e+01]],\n",
      "\n",
      "         [[ 5.5085e+00, -1.2394e+01,  4.9716e+00,  ..., -2.5163e+00,\n",
      "            3.9593e+00,  6.2856e+00],\n",
      "          [-1.2812e+01, -4.5736e+00,  7.0009e+00,  ...,  9.5676e+00,\n",
      "           -3.1642e+00,  2.7095e+00],\n",
      "          [-3.1381e+00,  3.5302e+00,  3.6626e+00,  ...,  5.1783e+00,\n",
      "           -1.0559e+00,  7.7431e+00],\n",
      "          ...,\n",
      "          [-1.9922e+01, -4.4083e+00, -1.6321e+00,  ...,  1.1761e+01,\n",
      "            2.0982e+00,  1.7132e+01],\n",
      "          [ 2.8660e+00,  7.9231e+00,  1.1497e+01,  ...,  2.4465e+01,\n",
      "            2.0669e+01,  1.1221e+01],\n",
      "          [ 6.3561e+00,  5.8087e+00,  1.2668e+01,  ...,  1.1883e+01,\n",
      "            5.5759e+00,  2.5336e+00]],\n",
      "\n",
      "         [[-4.5597e+01, -3.1468e+01, -3.6979e+01,  ..., -2.8953e+01,\n",
      "           -2.7107e+01, -2.2068e+01],\n",
      "          [-2.6073e+01, -2.1997e+01, -3.5567e+01,  ..., -2.7746e+01,\n",
      "           -4.6442e+01, -3.5851e+01],\n",
      "          [-2.6820e+01, -2.4481e+01, -3.8975e+01,  ..., -2.0478e+01,\n",
      "           -1.7301e+01, -3.1326e+01],\n",
      "          ...,\n",
      "          [-2.9219e+01, -2.7149e+01, -2.2627e+01,  ..., -3.0052e+01,\n",
      "           -5.8442e+01, -5.6105e+01],\n",
      "          [-3.4254e+01, -2.5342e+01, -1.9104e+01,  ..., -7.9953e+01,\n",
      "           -6.5974e+01, -3.9876e+01],\n",
      "          [-3.2368e+01, -3.0908e+01, -1.5540e+01,  ..., -5.0765e+01,\n",
      "           -4.2137e+01, -3.7684e+01]]]], grad_fn=<AddBackward0>), tensor([[[[ 3.1464e+01,  2.1562e+01,  7.6164e+00,  ...,  2.8008e+00,\n",
      "            7.2987e+00,  6.9083e+00],\n",
      "          [ 1.7503e+01,  9.5442e+00,  6.2223e+00,  ...,  5.3733e+00,\n",
      "           -1.6559e+00, -1.6324e-01],\n",
      "          [ 5.4026e+00,  1.0267e+01, -4.4785e+00,  ...,  1.6996e+01,\n",
      "            1.0795e+01,  1.9419e+00],\n",
      "          ...,\n",
      "          [ 7.2837e+00,  2.5010e+01,  1.5281e+01,  ...,  1.7055e+01,\n",
      "           -1.4878e+01,  1.8277e+01],\n",
      "          [ 5.8521e+00,  1.6736e+01,  1.2910e+01,  ...,  2.5843e+00,\n",
      "            1.0814e+01,  8.3172e+00],\n",
      "          [ 3.7585e+01,  1.9231e+01,  2.3760e+01,  ...,  3.7320e+01,\n",
      "            3.0444e+01,  1.9702e+01]],\n",
      "\n",
      "         [[-6.0020e+00,  1.2541e+01, -6.5647e+00,  ...,  1.0183e+01,\n",
      "            2.5194e+00,  1.2141e+01],\n",
      "          [ 8.6720e+00,  1.1669e+01,  8.6243e+00,  ...,  9.6541e+00,\n",
      "            5.5621e+00, -2.1662e+00],\n",
      "          [ 1.4404e+01,  3.0874e+01, -3.0776e+00,  ...,  1.2600e+01,\n",
      "            1.8092e+00, -3.8272e+00],\n",
      "          ...,\n",
      "          [ 1.0907e+00,  6.9438e+00,  1.0801e+01,  ..., -1.0651e+01,\n",
      "           -2.3013e+00,  1.0761e+01],\n",
      "          [-2.3551e+01, -1.1260e+01,  1.6497e+01,  ..., -7.7115e+00,\n",
      "            3.8849e-01,  2.1024e+01],\n",
      "          [ 1.0061e+00, -4.2777e+00,  6.3386e-02,  ...,  2.0533e+01,\n",
      "            9.1307e+00, -1.1498e+00]],\n",
      "\n",
      "         [[ 2.8197e+01,  1.6533e+01,  3.0120e+01,  ...,  2.6967e+01,\n",
      "            2.7803e+01,  8.9386e+00],\n",
      "          [ 2.0795e+01,  3.2674e+01,  3.2327e+01,  ...,  3.7393e+01,\n",
      "            4.4279e+01,  2.2490e+01],\n",
      "          [ 1.7163e+01,  3.0638e+01,  4.6337e+01,  ...,  3.3657e+01,\n",
      "            2.2148e+01,  3.0915e+00],\n",
      "          ...,\n",
      "          [ 3.1639e+01,  2.3316e+01,  3.0593e+01,  ...,  2.3421e+01,\n",
      "            3.5893e+01,  3.7042e+01],\n",
      "          [ 4.2015e+01,  3.3397e+01,  2.5127e+01,  ...,  6.1703e+01,\n",
      "            6.7508e+01,  2.9959e+01],\n",
      "          [ 5.7838e-01,  1.4085e+01,  9.5512e+00,  ...,  1.1279e+00,\n",
      "            4.2991e+00,  4.3622e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3397e+01, -3.2979e+01, -3.6228e+01,  ..., -2.6962e+01,\n",
      "           -1.5894e+01, -1.1806e+01],\n",
      "          [-1.4610e+01, -1.6562e+01, -2.6832e+01,  ..., -2.0598e+00,\n",
      "           -1.4046e+01, -4.8434e+00],\n",
      "          [-8.8319e+00, -2.5123e+01,  8.1711e+00,  ..., -1.6241e+01,\n",
      "           -9.5275e+00, -7.8047e+00],\n",
      "          ...,\n",
      "          [-2.4110e+01, -1.7831e+01, -1.9794e+01,  ..., -5.5355e+01,\n",
      "           -6.2220e+00, -3.9266e+01],\n",
      "          [-2.2556e+01, -2.1342e+01, -1.2079e+01,  ..., -3.5697e+01,\n",
      "           -5.0392e+01, -5.1353e+01],\n",
      "          [-3.8327e+01, -2.4464e+01, -1.1330e+01,  ..., -3.5739e+01,\n",
      "           -3.8630e+01, -1.5473e+01]],\n",
      "\n",
      "         [[ 1.3851e+01, -6.9585e+00,  1.4170e+01,  ...,  5.7888e+00,\n",
      "            1.2734e+01,  1.3409e+01],\n",
      "          [-7.4479e+00, -1.1829e-01,  1.1423e+01,  ...,  1.6661e+01,\n",
      "            3.0160e+00,  3.6034e+00],\n",
      "          [ 5.1188e-01,  8.1155e+00,  9.1225e+00,  ...,  6.4121e+00,\n",
      "           -1.2566e+00,  7.2588e+00],\n",
      "          ...,\n",
      "          [-1.4878e+01, -2.1378e-01, -5.8293e-01,  ...,  1.6238e+01,\n",
      "            9.4107e+00,  2.4906e+01],\n",
      "          [ 9.8472e+00,  1.3875e+01,  1.5773e+01,  ...,  3.3869e+01,\n",
      "            3.0294e+01,  1.9600e+01],\n",
      "          [ 1.1461e+01,  8.9892e+00,  1.6637e+01,  ...,  1.4176e+01,\n",
      "            7.1186e+00,  8.3651e+00]],\n",
      "\n",
      "         [[-5.2416e+01, -3.3602e+01, -4.0048e+01,  ..., -2.8430e+01,\n",
      "           -3.1145e+01, -2.9515e+01],\n",
      "          [-2.4168e+01, -2.5416e+01, -3.7121e+01,  ..., -2.4966e+01,\n",
      "           -4.0924e+01, -3.9663e+01],\n",
      "          [-2.6128e+01, -3.1185e+01, -3.8381e+01,  ..., -1.6788e+01,\n",
      "           -2.1878e+01, -3.2284e+01],\n",
      "          ...,\n",
      "          [-2.7182e+01, -2.6583e+01, -1.3279e+01,  ..., -2.8365e+01,\n",
      "           -5.9491e+01, -7.5375e+01],\n",
      "          [-3.5712e+01, -2.5621e+01, -1.8653e+01,  ..., -1.0194e+02,\n",
      "           -7.9861e+01, -7.4108e+01],\n",
      "          [-5.1543e+01, -4.8031e+01, -2.9242e+01,  ..., -7.5242e+01,\n",
      "           -7.1666e+01, -7.4065e+01]]]], grad_fn=<AddBackward0>), tensor([[[[ 9.7770e+00, -1.9134e+00, -4.1615e+00,  ...,  2.0553e+00,\n",
      "            2.8206e+00,  2.6942e+00],\n",
      "          [ 4.5117e+00,  8.9391e+00,  1.5943e+00,  ..., -2.4733e+00,\n",
      "            1.4638e+01,  1.2889e+01],\n",
      "          [ 5.4295e+00,  1.1101e+00,  6.0236e+00,  ...,  7.1549e-02,\n",
      "            5.1744e+00,  8.8771e+00],\n",
      "          ...,\n",
      "          [ 7.1702e+00, -2.6516e-01, -2.8521e+00,  ...,  8.3947e-01,\n",
      "            9.6979e+00,  5.0355e+00],\n",
      "          [ 1.2577e+01,  2.5390e-01, -4.4515e+00,  ...,  4.8414e+00,\n",
      "            6.1425e+00,  1.1142e+00],\n",
      "          [ 4.6503e+00,  4.0934e+00,  2.5144e+00,  ...,  1.3996e-01,\n",
      "           -5.2934e-01, -2.0860e+00]],\n",
      "\n",
      "         [[ 4.1792e+00, -2.3712e+00,  1.5351e+00,  ..., -2.0393e+00,\n",
      "           -3.0892e+00,  1.0055e+00],\n",
      "          [ 6.2834e+00,  6.5224e+00,  9.7276e-01,  ...,  2.5515e+00,\n",
      "            1.1165e+00,  1.8161e+00],\n",
      "          [ 3.4615e+00,  4.8523e+00,  6.1421e+00,  ..., -1.4982e+00,\n",
      "            3.5093e+00, -3.7596e-01],\n",
      "          ...,\n",
      "          [-1.5325e+00, -8.0543e+00, -1.7146e+00,  ...,  5.9014e-01,\n",
      "           -5.4031e+00, -4.4466e-02],\n",
      "          [ 2.8600e+00, -2.2800e+00, -3.1877e+00,  ...,  7.3385e+00,\n",
      "           -2.8857e+00,  4.8384e+00],\n",
      "          [ 1.1917e+01,  9.7448e+00,  6.6643e+00,  ...,  4.9116e+00,\n",
      "            6.1233e+00,  7.5369e+00]],\n",
      "\n",
      "         [[ 4.3683e+00,  3.0095e+00, -6.3434e-01,  ...,  5.1817e+00,\n",
      "            1.0696e+00,  7.3363e+00],\n",
      "          [ 3.2715e+00,  4.6969e+00,  4.5455e+00,  ...,  6.8435e+00,\n",
      "           -6.9193e+00, -5.9676e+00],\n",
      "          [ 1.7506e+00,  4.6072e+00,  1.2628e+00,  ...,  1.2010e+00,\n",
      "            1.9065e+00,  4.8660e+00],\n",
      "          ...,\n",
      "          [-8.4891e-01,  5.0866e+00,  9.1329e+00,  ...,  1.3309e+00,\n",
      "            2.8146e+00,  8.0477e+00],\n",
      "          [-5.1572e+00,  2.3797e+00,  8.4861e+00,  ...,  8.9222e-01,\n",
      "            3.2482e+00,  8.9537e+00],\n",
      "          [ 2.6132e+00,  6.8542e+00,  5.1431e+00,  ..., -4.9892e+00,\n",
      "           -4.5362e+00, -3.3441e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2700e+00, -4.3032e+00, -1.9505e+00,  ..., -7.1192e-01,\n",
      "           -2.5410e+00,  1.0920e+00],\n",
      "          [ 2.4864e+00, -5.0326e+00, -2.3055e+00,  ...,  1.5879e+00,\n",
      "            3.2772e+00,  5.5101e+00],\n",
      "          [ 1.3374e+00,  4.3878e+00,  4.3169e+00,  ...,  1.9456e+00,\n",
      "            3.4781e+00,  7.1254e+00],\n",
      "          ...,\n",
      "          [-1.0422e+01, -2.7923e+00, -8.6151e+00,  ..., -7.2951e+00,\n",
      "           -8.5663e+00, -1.0066e+00],\n",
      "          [ 1.9811e+00, -2.7077e+00, -5.7769e+00,  ..., -9.8815e-03,\n",
      "           -3.0095e+00,  1.6328e+00],\n",
      "          [ 4.0698e+00,  3.1691e+00,  8.9075e+00,  ..., -8.7344e-01,\n",
      "           -1.4005e+00,  2.9539e+00]],\n",
      "\n",
      "         [[ 7.1680e+00,  1.0079e+00, -5.7855e+00,  ..., -2.3668e+00,\n",
      "           -3.2583e+00, -3.8745e+00],\n",
      "          [ 3.8867e+00,  4.4824e+00, -1.6597e+00,  ...,  1.5802e+00,\n",
      "            1.2459e+01,  1.8308e+01],\n",
      "          [ 7.7094e+00,  2.0735e+00,  1.2498e+00,  ...,  6.5183e+00,\n",
      "            6.4921e+00,  6.8146e+00],\n",
      "          ...,\n",
      "          [-4.4963e+00,  2.5180e+00,  3.7736e+00,  ..., -3.2186e+00,\n",
      "           -1.8507e+00, -2.1323e-01],\n",
      "          [-5.1458e+00, -4.8480e+00, -4.7631e+00,  ...,  1.9131e-01,\n",
      "            2.5530e+00, -1.3847e+01],\n",
      "          [ 1.9365e+00, -1.4964e+00, -5.5185e+00,  ..., -4.9482e+00,\n",
      "           -1.0230e+01, -8.8954e+00]],\n",
      "\n",
      "         [[ 1.5717e+01, -3.0187e-01,  3.6294e+00,  ..., -4.8635e+00,\n",
      "            1.5876e+00,  6.8575e+00],\n",
      "          [-2.2307e+00,  1.5153e+00,  9.7160e+00,  ...,  1.7374e+00,\n",
      "            3.5663e+00,  9.1914e+00],\n",
      "          [ 2.8162e+00,  1.1464e+01,  8.8871e+00,  ...,  6.6490e+00,\n",
      "           -4.7144e-01,  2.2194e+00],\n",
      "          ...,\n",
      "          [ 1.1776e-01,  1.5671e+00,  3.1355e+00,  ...,  6.4122e+00,\n",
      "            7.7548e+00,  1.1838e+01],\n",
      "          [ 3.2348e+00,  8.2913e+00,  1.1429e+01,  ...,  1.2183e+01,\n",
      "            1.2773e+01,  4.9893e+00],\n",
      "          [ 1.9963e+01,  1.5150e+01,  1.3888e+01,  ...,  6.7441e+00,\n",
      "            3.4204e+00,  3.9562e+00]]]], grad_fn=<NativeBatchNormBackward0>), tensor([[[[  8.6225,  -1.1645,  -4.8508,  ...,   3.8650,   4.5792,   3.0581],\n",
      "          [  5.2791,   8.5567,   0.5662,  ...,  -2.0318,  14.6548,  12.6235],\n",
      "          [  5.5599,  -1.1746,   4.5152,  ...,  -0.0399,   4.2672,   8.5533],\n",
      "          ...,\n",
      "          [  6.1688,  -0.1270,  -1.9654,  ...,   1.1410,  12.4390,   8.6242],\n",
      "          [ 13.1779,   1.3909,  -4.0468,  ...,   3.7775,   6.6288,   2.5074],\n",
      "          [  3.8771,   4.2998,   2.9067,  ...,   1.5040,   1.1683,  -1.3459]],\n",
      "\n",
      "         [[  3.0642,  -2.7953,   1.6959,  ...,  -2.2413,  -3.8348,   0.1986],\n",
      "          [  6.3887,   6.0318,   0.6810,  ...,  -0.1196,  -2.8890,  -2.8136],\n",
      "          [  3.0074,   4.0338,   3.0329,  ...,  -5.5575,   1.8642,  -1.6396],\n",
      "          ...,\n",
      "          [ -3.2354,  -9.6931,  -2.1053,  ...,   1.4416,  -4.9453,  -1.8100],\n",
      "          [  0.7625,  -4.0434,  -5.4373,  ...,   5.1467,  -4.8422,   2.7117],\n",
      "          [ 10.3933,   6.9881,   4.7288,  ...,   4.1636,   5.4410,   6.5337]],\n",
      "\n",
      "         [[  1.2383,   2.8308,  -3.4878,  ...,   4.1852,   2.0968,   8.4178],\n",
      "          [  2.5288,   1.8049,   1.3859,  ...,   3.3711,  -5.7398,  -4.9132],\n",
      "          [  1.7275,   1.4841,  -0.2905,  ...,   2.3993,   1.8025,   4.3313],\n",
      "          ...,\n",
      "          [ -6.5683,  -0.1056,   8.0308,  ...,   0.5301,   1.2096,   2.7457],\n",
      "          [ -9.0190,  -2.5601,   5.5369,  ...,  -0.1549,  -1.9241,   3.9761],\n",
      "          [ -3.0681,  -0.2885,  -0.3524,  ...,  -4.8265,  -4.1256,  -2.2787]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  2.3313,  -6.7294,  -4.2972,  ...,  -3.9437,  -6.6179,  -2.0756],\n",
      "          [  1.5222,  -6.1659,  -4.2874,  ...,  -2.1875,   3.3305,   6.8321],\n",
      "          [ -0.3895,   2.8061,   0.2178,  ...,  -0.1116,   1.1525,   6.1497],\n",
      "          ...,\n",
      "          [-10.0326,  -3.5999, -10.4367,  ...,  -8.2204,  -7.9984,  -1.3337],\n",
      "          [  3.0982,  -2.3074,  -5.7803,  ...,   0.0623,  -2.9307,   2.5387],\n",
      "          [  5.7532,   4.4841,   9.8088,  ...,   2.3307,   1.9182,   4.9701]],\n",
      "\n",
      "         [[  4.1146,   3.4087,  -8.6169,  ...,  -3.9308,  -1.7477,  -3.2557],\n",
      "          [  5.5678,   3.7006,  -4.6560,  ...,   0.1034,  12.2692,  18.2379],\n",
      "          [ 10.9495,   1.1046,   1.5531,  ...,   9.1078,   7.3387,   6.7533],\n",
      "          ...,\n",
      "          [ -7.4537,  -2.2860,   1.1612,  ...,  -5.4177,  -3.0147,  -2.5508],\n",
      "          [ -8.2158, -10.6409, -10.4681,  ...,  -3.2372,  -0.8904, -16.4359],\n",
      "          [ -3.6478,  -8.2456, -12.8813,  ...,  -2.6052,  -7.7585,  -9.3274]],\n",
      "\n",
      "         [[ 20.3222,   2.6481,   4.4027,  ...,  -3.4967,   3.8140,  10.6377],\n",
      "          [ -0.2326,   0.2643,   7.8908,  ...,   0.0259,   9.3040,  10.3168],\n",
      "          [  4.3629,  11.4948,  10.4408,  ...,  10.1224,   3.3654,   1.0805],\n",
      "          ...,\n",
      "          [ -0.2911,   1.5271,   1.8075,  ...,   3.5634,   8.1529,   5.3711],\n",
      "          [ -0.4541,   5.6240,  10.7823,  ...,   5.6648,   7.0902,  -3.1906],\n",
      "          [ 12.9221,   6.3289,   9.2321,  ...,   2.5667,  -0.2983,  -0.9240]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[  7.6315,  -1.9481,  -7.0316,  ...,   2.7649,   3.8296,   2.3886],\n",
      "          [  4.9063,   9.1077,   0.0651,  ...,  -3.3061,  14.6568,  11.3028],\n",
      "          [  5.7884,  -0.6773,   4.1788,  ...,  -0.9043,   4.7439,   7.6490],\n",
      "          ...,\n",
      "          [  5.5015,  -1.2613,  -1.7435,  ...,  -0.0314,  12.2910,  10.5099],\n",
      "          [ 11.6128,   1.1716,  -4.8625,  ...,   2.7074,   6.7595,   3.5167],\n",
      "          [  2.0168,   3.1573,   1.2994,  ...,   1.2949,   1.0105,  -2.1295]],\n",
      "\n",
      "         [[  3.0072,  -2.5736,   1.2741,  ...,  -3.1797,  -4.5524,  -0.1441],\n",
      "          [  7.3297,   6.0565,  -0.6982,  ...,  -2.2960,  -3.3099,  -3.1618],\n",
      "          [  3.0374,   3.2429,   2.5308,  ...,  -8.8201,   0.0716,  -3.1371],\n",
      "          ...,\n",
      "          [ -3.1118, -10.1977,  -2.5230,  ...,   0.8207,  -3.8931,  -1.5308],\n",
      "          [  1.2324,  -4.7025,  -6.9875,  ...,   5.2521,  -4.9207,   0.9262],\n",
      "          [  8.7975,   4.9436,   1.3377,  ...,   2.2560,   3.6011,   3.5052]],\n",
      "\n",
      "         [[  0.4977,   1.6315,  -5.1668,  ...,   2.7231,   0.7922,   7.1109],\n",
      "          [  1.5150,   1.3850,   1.2698,  ...,   2.3222,  -5.9773,  -5.1483],\n",
      "          [  0.8463,   2.4290,   2.8005,  ...,   0.6309,   1.0781,   3.9493],\n",
      "          ...,\n",
      "          [ -6.3708,  -0.1309,   8.7244,  ...,   0.1643,   2.2108,   2.5651],\n",
      "          [ -9.3435,  -2.8330,   5.5133,  ...,   1.4935,   0.0487,   3.5997],\n",
      "          [ -3.8288,  -0.8441,  -1.2942,  ...,  -3.5161,  -2.3640,  -1.3360]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  3.4176,  -7.1062,  -4.5483,  ...,  -3.8194,  -6.8632,  -2.5543],\n",
      "          [  1.1424,  -7.3214,  -3.4480,  ...,  -2.7840,   3.3666,   7.0661],\n",
      "          [ -1.0897,   2.5649,   1.5910,  ...,  -0.4930,   1.7757,   7.0457],\n",
      "          ...,\n",
      "          [-11.3778,  -2.9808,  -7.8232,  ...,  -7.2664,  -7.2866,  -0.1223],\n",
      "          [  3.5327,  -1.9297,  -3.9494,  ...,   1.3759,  -1.3759,   3.9322],\n",
      "          [  4.6654,   3.1584,   8.0335,  ...,   2.1131,   2.4029,   5.8049]],\n",
      "\n",
      "         [[  2.4317,   3.5698,  -9.7842,  ...,  -6.6452,  -4.4820,  -4.3430],\n",
      "          [  3.6685,   0.8106,  -5.7298,  ...,  -4.3025,   8.1770,  16.0122],\n",
      "          [ 10.4150,   1.1348,   1.5487,  ...,   8.2195,   5.9574,   5.8482],\n",
      "          ...,\n",
      "          [ -9.5953,  -2.6456,   2.1492,  ...,  -7.2591,  -3.6484,  -5.0869],\n",
      "          [-10.9166, -12.1739,  -9.4558,  ...,  -2.4300,   0.2163, -16.6551],\n",
      "          [ -5.7582,  -9.3865, -17.0978,  ...,  -0.6632,  -5.8145,  -8.2807]],\n",
      "\n",
      "         [[ 14.7166,  -0.2247,   2.2860,  ...,  -5.6429,   1.2471,   5.2492],\n",
      "          [ -1.4609,   2.1560,   7.4836,  ...,   0.9401,  11.8116,   9.0821],\n",
      "          [  3.7971,  13.1493,   9.6864,  ...,  11.6253,   4.1102,   0.7972],\n",
      "          ...,\n",
      "          [ -3.4147,   0.7812,   3.1219,  ...,   2.3053,   8.7069,   3.6407],\n",
      "          [ -3.3882,   5.1352,  10.0037,  ...,   6.0325,   7.9426,  -4.5242],\n",
      "          [  7.1799,   5.5387,   5.7087,  ...,   1.6087,  -1.7080,  -2.8732]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[  2.6162,   6.5908,   2.8846,  ...,  -3.8984,   0.7610,  -4.6540],\n",
      "          [  4.2386,   2.7411,   1.1117,  ...,   2.5579,   1.2695,   0.4970],\n",
      "          [  6.4894,   4.4674,  -0.5393,  ...,   5.6373,  -0.7712,   2.3940],\n",
      "          ...,\n",
      "          [  1.7395,   1.3098,   6.2226,  ...,   4.2456,   2.2077,   3.9170],\n",
      "          [  3.0565,   5.3321,   3.6758,  ...,   6.6194,   0.3562,   1.0700],\n",
      "          [ -2.4246,  -0.2328,  -3.4873,  ...,  -0.3975,  -2.3036,  -5.7916]],\n",
      "\n",
      "         [[  0.5568,  -1.9283,   1.3319,  ...,  -1.3874,  -1.1432,  -4.0616],\n",
      "          [ -3.2554,  -3.6382,  -3.0670,  ...,  -2.9654,  -2.2193,  -3.9777],\n",
      "          [ -7.1263,  -3.2201,  -3.3734,  ...,  -3.6751,  -8.9664,  -7.6366],\n",
      "          ...,\n",
      "          [ -1.6473,  -3.7599,   0.2777,  ...,  -3.6879,  -4.8703,  -2.6791],\n",
      "          [ -1.0720,  -3.1051,  -0.9973,  ...,  -2.2161,  -1.9566,   2.9292],\n",
      "          [  4.3832,   2.7394,  -1.7882,  ...,   1.0065,  -0.4107,   2.2081]],\n",
      "\n",
      "         [[  3.0493,  -0.1811,  -1.9542,  ..., -10.6308,  -6.4542,  -0.7790],\n",
      "          [ -5.6122,  -5.3302,  -7.4931,  ...,  -7.0455,  -9.4689,  -4.6246],\n",
      "          [ -6.1724,  -3.0455,  -9.4435,  ...,  -2.0357,  -7.3847,  -2.4788],\n",
      "          ...,\n",
      "          [ -6.2075,  -6.3013,  -9.2414,  ...,  -7.5902,  -6.5711,  -7.9163],\n",
      "          [-10.8041,  -7.5413,  -7.1266,  ..., -12.5833, -10.2935,  -7.2851],\n",
      "          [ -5.5709,  -5.6080,  -4.7910,  ...,  -7.0838,  -3.3599,  -1.6912]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  5.7542,  12.1308,  13.3468,  ...,   7.0010,   7.6904,   9.5346],\n",
      "          [  6.2733,  10.6069,  14.1235,  ...,   8.3360,   5.6523,   5.4625],\n",
      "          [  6.1123,  10.7500,  11.8651,  ...,  10.3657,  11.4805,   7.8944],\n",
      "          ...,\n",
      "          [  5.6597,   9.8782,   8.0054,  ...,   8.0159,   7.0369,   3.8027],\n",
      "          [ -0.3416,   5.2827,   8.6879,  ...,   4.7450,   6.6322,   9.5565],\n",
      "          [  5.2162,   4.0078,   5.2625,  ...,   3.4245,   6.1680,   9.9053]],\n",
      "\n",
      "         [[ 12.6751,  10.5110,   7.0278,  ...,   7.9425,  11.2758,   9.4549],\n",
      "          [ 10.2589,   1.6290,   5.5118,  ...,  10.5540,   8.9406,   6.6531],\n",
      "          [  9.5138,   7.9148,  11.7251,  ...,  10.8672,  10.1246,  11.6799],\n",
      "          ...,\n",
      "          [  8.4708,   3.0925,   4.0312,  ...,  13.3268,  13.1134,  11.3340],\n",
      "          [  9.0194,   5.2200,   4.9875,  ...,   5.2172,   6.6378,   8.4470],\n",
      "          [ 11.6366,   8.5455,   9.9720,  ...,   6.6923,   6.9574,  12.8180]],\n",
      "\n",
      "         [[ -7.2209,  -5.3808,  -8.0579,  ...,  -4.9983,  -5.0184,  -8.3338],\n",
      "          [ -2.6386,  -3.7091,  -3.5249,  ...,  -0.8591,  -1.3034,  -5.5429],\n",
      "          [ -2.7346,  -1.1935,  -1.6049,  ...,   1.9110,  -0.6007,  -2.6371],\n",
      "          ...,\n",
      "          [ -0.3125,   0.0374,  -1.6715,  ...,  -2.8582,  -3.9563,  -4.4129],\n",
      "          [ -2.3976,   1.0378,  -1.9908,  ...,  -1.9414,  -3.5552,  -5.3846],\n",
      "          [ -9.8890,  -3.9969,  -5.9406,  ...,  -4.8833,  -5.1334, -12.3608]]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>), tensor([[[[ 2.3070e+00,  5.9360e+00,  2.6534e+00,  ..., -4.5198e+00,\n",
      "            9.3483e-01, -4.4104e+00],\n",
      "          [ 1.9213e+00, -7.3283e-02, -1.0647e-01,  ...,  1.7441e+00,\n",
      "            8.3505e-01,  3.2378e-01],\n",
      "          [ 4.4639e+00,  2.3315e+00, -2.7180e+00,  ...,  3.4044e+00,\n",
      "           -2.8976e+00,  9.1704e-01],\n",
      "          ...,\n",
      "          [ 5.6155e-02, -4.9938e-01,  4.2143e+00,  ...,  1.4108e+00,\n",
      "           -3.2391e-01,  2.7819e+00],\n",
      "          [ 2.1324e+00,  4.5101e+00,  2.2072e+00,  ...,  5.3379e+00,\n",
      "           -2.0298e-01,  5.8966e-01],\n",
      "          [-2.3921e+00, -5.9468e-02, -3.6195e+00,  ...,  1.2002e-01,\n",
      "           -2.5855e+00, -6.4458e+00]],\n",
      "\n",
      "         [[-1.0979e+00, -2.2697e+00,  1.4773e+00,  ..., -2.0196e+00,\n",
      "           -1.0061e+00, -3.8085e+00],\n",
      "          [-3.3444e+00, -2.9685e+00, -2.7936e+00,  ..., -3.4804e+00,\n",
      "           -2.5491e+00, -3.6579e+00],\n",
      "          [-7.2372e+00, -4.0821e+00, -4.9070e+00,  ..., -3.7195e+00,\n",
      "           -1.0155e+01, -7.3426e+00],\n",
      "          ...,\n",
      "          [-8.8631e-01, -3.9414e+00, -1.1462e+00,  ..., -2.9348e+00,\n",
      "           -5.0098e+00, -2.4128e+00],\n",
      "          [-4.1511e-01, -3.9169e+00, -2.1577e+00,  ..., -2.3656e+00,\n",
      "           -2.2668e+00,  2.8840e+00],\n",
      "          [ 3.8383e+00,  2.7072e+00, -1.5636e+00,  ...,  4.2722e-01,\n",
      "           -1.0376e-01,  2.2932e+00]],\n",
      "\n",
      "         [[ 9.3301e-01, -1.4083e+00, -4.4434e+00,  ..., -1.2208e+01,\n",
      "           -7.4558e+00, -2.6082e+00],\n",
      "          [-5.5443e+00, -6.8908e+00, -1.1983e+01,  ..., -8.5707e+00,\n",
      "           -1.0800e+01, -7.1939e+00],\n",
      "          [-6.6207e+00, -4.1257e+00, -1.3501e+01,  ..., -3.9205e+00,\n",
      "           -9.9812e+00, -5.2908e+00],\n",
      "          ...,\n",
      "          [-8.2767e+00, -8.0224e+00, -1.2224e+01,  ..., -1.0801e+01,\n",
      "           -8.9538e+00, -9.9345e+00],\n",
      "          [-1.2961e+01, -9.3922e+00, -9.1930e+00,  ..., -1.4780e+01,\n",
      "           -1.1644e+01, -9.5411e+00],\n",
      "          [-8.3305e+00, -7.0698e+00, -6.3356e+00,  ..., -8.6205e+00,\n",
      "           -5.0316e+00, -3.5783e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.1316e+00,  1.5085e+01,  1.3311e+01,  ...,  8.0509e+00,\n",
      "            8.5443e+00,  1.0726e+01],\n",
      "          [ 7.4758e+00,  1.0611e+01,  1.1711e+01,  ...,  9.0557e+00,\n",
      "            5.7816e+00,  5.9540e+00],\n",
      "          [ 6.6477e+00,  9.5403e+00,  1.0783e+01,  ...,  1.0250e+01,\n",
      "            1.1818e+01,  7.1693e+00],\n",
      "          ...,\n",
      "          [ 6.5294e+00,  9.5063e+00,  7.3433e+00,  ...,  7.2280e+00,\n",
      "            6.8157e+00,  3.7125e+00],\n",
      "          [ 8.4907e-01,  5.7606e+00,  9.3190e+00,  ...,  4.1881e+00,\n",
      "            7.2641e+00,  1.0183e+01],\n",
      "          [ 7.1784e+00,  5.9565e+00,  6.9206e+00,  ...,  4.2830e+00,\n",
      "            7.1037e+00,  1.1168e+01]],\n",
      "\n",
      "         [[ 1.1383e+01,  9.9751e+00,  4.7296e+00,  ...,  4.7803e+00,\n",
      "            9.6734e+00,  7.8866e+00],\n",
      "          [ 8.8315e+00, -1.2665e+00,  1.8127e+00,  ...,  7.1570e+00,\n",
      "            6.5900e+00,  4.1915e+00],\n",
      "          [ 7.7122e+00,  4.6577e+00,  6.0581e+00,  ...,  7.4366e+00,\n",
      "            6.3485e+00,  8.2495e+00],\n",
      "          ...,\n",
      "          [ 5.7468e+00,  4.2946e-02, -1.1347e+00,  ...,  9.2356e+00,\n",
      "            1.0005e+01,  8.1064e+00],\n",
      "          [ 6.9310e+00,  3.1038e+00,  2.5654e+00,  ...,  2.8439e+00,\n",
      "            5.1554e+00,  7.3479e+00],\n",
      "          [ 1.0987e+01,  7.9893e+00,  9.6966e+00,  ...,  4.9928e+00,\n",
      "            6.4898e+00,  1.2686e+01]],\n",
      "\n",
      "         [[-6.5250e+00, -6.5059e+00, -9.1310e+00,  ..., -5.3123e+00,\n",
      "           -4.3288e+00, -7.7220e+00],\n",
      "          [-2.2935e+00, -2.3981e+00, -2.8568e+00,  ..., -8.4738e-01,\n",
      "           -1.2328e+00, -6.1451e+00],\n",
      "          [-2.1032e+00,  3.9442e-03, -2.2787e+00,  ...,  2.7410e+00,\n",
      "            2.6768e-01, -3.7719e+00],\n",
      "          ...,\n",
      "          [ 5.3668e-01,  1.0042e+00, -1.2173e+00,  ..., -2.5032e+00,\n",
      "           -3.4897e+00, -5.5434e+00],\n",
      "          [-1.8446e+00,  2.9828e+00, -6.6679e-02,  ..., -1.8227e+00,\n",
      "           -3.0495e+00, -6.4820e+00],\n",
      "          [-9.8669e+00, -3.6009e+00, -6.5247e+00,  ..., -5.4471e+00,\n",
      "           -5.6478e+00, -1.2888e+01]]]], grad_fn=<AddBackward0>), tensor([[[[  3.2026,   6.2002,   2.9163,  ...,  -3.9762,   1.8885,  -3.7675],\n",
      "          [  3.4765,   1.8200,   1.8026,  ...,   3.9153,   3.3125,   1.2522],\n",
      "          [  6.4220,   4.3560,  -1.2863,  ...,   5.5099,  -0.8008,   2.1173],\n",
      "          ...,\n",
      "          [  1.6270,   1.2880,   5.6760,  ...,   3.2405,   1.9999,   4.1031],\n",
      "          [  3.4338,   6.1690,   3.8953,  ...,   7.2041,   1.7185,   1.7735],\n",
      "          [ -2.3203,   0.5049,  -2.8296,  ...,   0.4533,  -1.9611,  -7.0448]],\n",
      "\n",
      "         [[ -1.5638,  -3.3670,   0.1430,  ...,  -3.5069,  -1.7559,  -4.5452],\n",
      "          [ -3.7539,  -2.8102,  -3.9164,  ...,  -4.4300,  -2.9588,  -5.1136],\n",
      "          [ -7.1534,  -5.1924,  -5.5234,  ...,  -4.0550, -10.9784,  -8.6264],\n",
      "          ...,\n",
      "          [ -1.5125,  -4.4944,  -2.2968,  ...,  -3.1224,  -6.3597,  -3.6521],\n",
      "          [ -1.4196,  -4.8207,  -3.5460,  ...,  -4.0836,  -3.1658,   1.4320],\n",
      "          [  2.6485,   2.1209,  -2.4082,  ...,  -0.5023,  -0.5262,   1.8496]],\n",
      "\n",
      "         [[  0.5033,  -1.3757,  -3.2294,  ..., -11.1512,  -7.0164,  -2.4451],\n",
      "          [ -6.0897,  -6.5460, -10.5484,  ...,  -8.5488, -10.2725,  -7.2903],\n",
      "          [ -7.2256,  -3.9305, -12.9465,  ...,  -4.0639,  -9.1733,  -5.2048],\n",
      "          ...,\n",
      "          [ -8.6835,  -8.2167, -11.6298,  ..., -10.8990,  -8.0182,  -9.6539],\n",
      "          [-12.7191,  -9.2056,  -8.7862,  ..., -13.9089,  -9.9766,  -9.3793],\n",
      "          [ -8.5426,  -7.3696,  -6.5166,  ...,  -9.0041,  -4.8790,  -3.8192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  8.4777,  15.3118,  13.5678,  ...,   8.6220,   9.0268,  10.5922],\n",
      "          [  7.9009,  11.2823,  10.9032,  ...,   9.7852,   5.9864,   5.6350],\n",
      "          [  8.0246,  10.0171,  10.0699,  ...,  10.9284,  10.9547,   6.6083],\n",
      "          ...,\n",
      "          [  6.6846,   9.0217,   6.8324,  ...,   7.4994,   6.0531,   3.6616],\n",
      "          [  0.8544,   5.5115,   8.9898,  ...,   4.2306,   6.8392,   9.4740],\n",
      "          [  6.9868,   5.9918,   7.0038,  ...,   3.8220,   6.5973,  10.0810]],\n",
      "\n",
      "         [[ 10.9984,  10.3030,   5.0672,  ...,   4.6444,   9.1619,   7.7197],\n",
      "          [  8.5184,  -1.1419,   1.6340,  ...,   6.5224,   6.2599,   3.8904],\n",
      "          [  7.5967,   4.2689,   5.4540,  ...,   6.4059,   5.6191,   7.5703],\n",
      "          ...,\n",
      "          [  5.2281,  -0.4100,  -1.6995,  ...,   7.7370,   8.8937,   7.6608],\n",
      "          [  6.5968,   2.5815,   1.7230,  ...,   1.6379,   5.0084,   6.6809],\n",
      "          [ 10.7314,   7.6797,   9.4871,  ...,   4.1934,   6.1365,  11.9493]],\n",
      "\n",
      "         [[ -7.1150,  -6.8140,  -8.3290,  ...,  -4.8942,  -4.5838,  -8.5921],\n",
      "          [ -3.2483,  -2.5550,  -3.5869,  ...,  -0.0987,  -2.1801,  -6.8219],\n",
      "          [ -3.1075,  -0.1960,  -3.1087,  ...,   2.2321,  -0.7296,  -4.5109],\n",
      "          ...,\n",
      "          [  0.1784,   0.8518,  -1.0411,  ...,  -3.2839,  -3.7743,  -5.6460],\n",
      "          [ -2.2044,   2.6045,  -0.4597,  ...,  -2.7831,  -3.2183,  -6.1057],\n",
      "          [ -9.9069,  -3.5120,  -6.6837,  ...,  -5.9256,  -6.1534, -13.6206]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[  1.7284,   5.3761,   2.3896,  ...,  -3.5492,   1.3956,  -4.6382],\n",
      "          [  3.2606,   1.1867,   3.1614,  ...,   3.8764,   2.6394,   1.2056],\n",
      "          [  5.8345,   3.7588,   0.5715,  ...,   4.7273,  -1.0576,   2.5169],\n",
      "          ...,\n",
      "          [  1.5109,   1.0053,   5.2571,  ...,   2.8687,   1.8740,   3.9540],\n",
      "          [  2.8667,   5.2520,   2.9904,  ...,   6.2419,   1.2157,   1.8054],\n",
      "          [ -5.3396,  -1.6898,  -5.4707,  ...,  -2.3334,  -3.7790, -10.5473]],\n",
      "\n",
      "         [[ -2.2379,  -3.4404,  -0.2907,  ...,  -2.9435,  -1.7485,  -4.6562],\n",
      "          [ -3.8011,  -2.3099,  -3.2141,  ...,  -3.5629,  -2.8416,  -5.0956],\n",
      "          [ -7.1388,  -5.4895,  -4.1901,  ...,  -3.9688, -10.4899,  -8.7617],\n",
      "          ...,\n",
      "          [ -1.5141,  -3.9577,  -2.3242,  ...,  -3.3492,  -6.4960,  -3.9631],\n",
      "          [ -2.1834,  -5.0128,  -3.7188,  ...,  -4.5075,  -3.2619,   1.1530],\n",
      "          [  1.9983,   1.1294,  -3.6246,  ...,  -1.5837,  -1.0847,   1.7661]],\n",
      "\n",
      "         [[ -0.1484,  -1.6639,  -3.0553,  ..., -11.3875,  -7.5014,  -4.2162],\n",
      "          [ -5.9121,  -6.7240,  -9.9257,  ...,  -7.4875, -10.2506,  -8.0065],\n",
      "          [ -7.0199,  -4.4298, -12.2882,  ...,  -2.9559,  -8.9612,  -5.6428],\n",
      "          ...,\n",
      "          [ -9.0448,  -8.1320, -10.8776,  ...,  -9.6338,  -6.7965,  -9.6528],\n",
      "          [-13.5750,  -9.6476,  -8.1633,  ..., -12.6106,  -9.2333,  -9.2403],\n",
      "          [ -9.5635,  -8.2182,  -7.3883,  ...,  -9.2447,  -5.1916,  -5.8353]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7.4048,  14.7181,  13.8323,  ...,   7.2961,   8.2475,   9.6845],\n",
      "          [  8.2966,  12.5815,  12.7071,  ...,  10.3713,   6.3956,   5.6508],\n",
      "          [  8.2585,  11.2647,  11.9073,  ...,  12.0575,  11.7394,   6.5163],\n",
      "          ...,\n",
      "          [  6.8419,   9.6939,   8.2287,  ...,   7.9880,   6.1192,   3.1922],\n",
      "          [  0.3620,   5.4587,   9.1348,  ...,   4.2410,   6.1944,   8.3731],\n",
      "          [  4.5646,   4.4235,   5.4877,  ...,   2.2693,   4.7449,   7.0515]],\n",
      "\n",
      "         [[ 12.5577,  11.2869,   6.0490,  ...,   6.3714,  10.5914,   8.9125],\n",
      "          [  9.5788,  -0.1390,   1.7368,  ...,   8.7052,   7.8113,   4.9471],\n",
      "          [  8.4723,   4.7935,   4.9762,  ...,   7.9344,   6.8908,   8.3649],\n",
      "          ...,\n",
      "          [  5.1280,  -0.1615,  -2.3024,  ...,   7.1448,   8.8204,   7.0071],\n",
      "          [  6.4849,   2.7496,   1.3277,  ...,   0.7514,   4.6766,   6.6843],\n",
      "          [ 11.9202,   8.0357,   9.2496,  ...,   3.4872,   5.9209,  12.7103]],\n",
      "\n",
      "         [[ -6.6380,  -5.1451,  -6.2298,  ...,  -3.8782,  -3.8256,  -9.2548],\n",
      "          [ -2.4087,  -1.1582,  -0.7420,  ...,   2.0696,  -0.7413,  -5.9335],\n",
      "          [ -2.1827,   1.6113,   0.6697,  ...,   5.9161,   1.2281,  -3.1419],\n",
      "          ...,\n",
      "          [  1.6773,   2.4670,   1.6005,  ...,  -0.1533,  -1.6824,  -4.9804],\n",
      "          [ -1.9345,   3.9408,   1.6971,  ...,  -0.6363,  -2.2777,  -6.1923],\n",
      "          [-10.1332,  -2.4856,  -5.7042,  ...,  -4.8461,  -5.4233, -14.7593]]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[[ 1.3623,  1.9110, -0.2971,  ...,  2.0072,  0.9957, -0.5162],\n",
      "          [ 1.4077, -0.1174, -4.2559,  ...,  0.0240,  0.3897, -0.1795],\n",
      "          [ 0.3471, -0.3065, -3.6010,  ..., -0.0382,  0.5247,  0.1316],\n",
      "          ...,\n",
      "          [-0.9245, -1.6510, -2.2638,  ..., -1.0941, -1.5923, -0.9488],\n",
      "          [-0.0323, -0.8034, -1.3807,  ..., -1.6238, -0.6356,  1.8714],\n",
      "          [ 0.0137,  0.2680, -0.1949,  ...,  0.6750,  2.6277,  1.7099]],\n",
      "\n",
      "         [[-2.2826, -1.3781, -0.1657,  ..., -2.3380, -2.4315, -3.8404],\n",
      "          [-1.5700,  0.3235,  5.9319,  ..., -0.0821, -1.2977, -1.9136],\n",
      "          [-1.1270,  1.8650,  5.7547,  ...,  0.4334, -0.0299, -1.2000],\n",
      "          ...,\n",
      "          [ 0.1866,  1.7234,  2.5275,  ...,  0.7121,  0.4127, -1.2972],\n",
      "          [-0.5653,  0.5393,  1.0104,  ...,  1.5064, -0.3721, -3.2705],\n",
      "          [-2.8665, -2.1942, -1.6875,  ...,  0.4803, -2.3631, -1.3719]],\n",
      "\n",
      "         [[-4.2406, -2.9649, -0.7805,  ..., -1.1453, -2.6739, -3.4434],\n",
      "          [-2.9128, -1.4738, -2.0857,  ..., -3.4906, -2.6259, -2.6025],\n",
      "          [-2.6596, -2.8318, -5.8056,  ..., -6.1532, -4.8825, -3.7577],\n",
      "          ...,\n",
      "          [-2.5895, -3.7253, -4.7877,  ..., -4.4135, -3.1512, -2.6713],\n",
      "          [-2.5623, -2.9967, -3.1662,  ..., -2.6024, -1.5519, -2.6205],\n",
      "          [-3.2358, -3.4600, -3.1969,  ..., -2.6593, -3.1910, -3.4123]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2298, -0.4706,  0.4519,  ...,  0.1598, -0.3484,  0.0297],\n",
      "          [-0.4800, -0.8945,  0.1828,  ..., -1.7360, -1.0068, -0.7115],\n",
      "          [ 0.0671, -0.9639, -1.2557,  ..., -2.5061, -1.5492, -1.1136],\n",
      "          ...,\n",
      "          [ 0.8253, -0.3705, -1.3605,  ..., -1.2361, -0.7800, -0.1157],\n",
      "          [ 1.4121,  0.2530, -0.3588,  ...,  0.4481,  1.0806,  0.9860],\n",
      "          [ 1.0346,  0.4850,  0.5539,  ...,  1.3748,  1.4932, -0.5493]],\n",
      "\n",
      "         [[-1.6549, -1.6060, -0.8132,  ..., -0.5955,  0.1618, -0.4872],\n",
      "          [-0.4094, -0.1499,  1.2847,  ..., -0.3753, -0.6511, -0.6852],\n",
      "          [-0.6835,  0.5224,  1.1698,  ..., -0.5801, -1.5549, -1.1475],\n",
      "          ...,\n",
      "          [ 0.0825,  0.7330,  1.0172,  ..., -0.2145, -0.7843, -0.4723],\n",
      "          [-0.7345, -1.1701, -0.9176,  ..., -0.4312, -0.6204, -0.4287],\n",
      "          [-1.2419, -0.6141, -0.8882,  ...,  0.1651,  0.2657, -1.9248]],\n",
      "\n",
      "         [[ 2.2853,  0.9469,  0.4892,  ..., -0.3355,  0.2516,  2.2966],\n",
      "          [ 1.7050,  0.0149, -1.6024,  ..., -1.3964, -1.2624,  0.8100],\n",
      "          [ 2.1941, -0.5291, -1.5793,  ..., -0.7766, -0.5905,  1.0844],\n",
      "          ...,\n",
      "          [ 2.3681,  0.5988, -1.5470,  ..., -1.8496, -1.2465,  0.4966],\n",
      "          [ 1.4406, -0.4483, -1.6098,  ..., -2.4617, -2.8057, -0.8044],\n",
      "          [ 2.5639,  0.9313,  0.3772,  ..., -0.9159, -0.8435,  1.0773]]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)]\n",
      "[[[0.65217394 0.43083003 0.44071147]\n",
      "  [0.65217394 0.43478262 0.44071147]\n",
      "  [0.6482214  0.43280634 0.4367589 ]\n",
      "  ...\n",
      "  [0.56521744 0.24901186 0.3339921 ]\n",
      "  [0.5335969  0.22529644 0.32015812]\n",
      "  [0.5237155  0.19169961 0.30632412]]\n",
      "\n",
      " [[0.66007906 0.4466403  0.45059288]\n",
      "  [0.66007906 0.4387352  0.4387352 ]\n",
      "  [0.66600794 0.44071147 0.4387352 ]\n",
      "  ...\n",
      "  [0.5513834  0.24110672 0.33003953]\n",
      "  [0.54347825 0.22727273 0.31422925]\n",
      "  [0.53952575 0.23913044 0.32015812]]\n",
      "\n",
      " [[0.6739131  0.4367589  0.44071147]\n",
      "  [0.68577075 0.4367589  0.4387352 ]\n",
      "  [0.6916996  0.4367589  0.44071147]\n",
      "  ...\n",
      "  [0.596838   0.22924902 0.3221344 ]\n",
      "  [0.64229256 0.28063244 0.33596838]\n",
      "  [0.66996044 0.34980237 0.39130434]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.64624506 0.4110672  0.41699606]\n",
      "  [0.63833994 0.4090909  0.41304347]\n",
      "  [0.6581028  0.40316206 0.4110672 ]\n",
      "  ...\n",
      "  [0.91106725 0.701581   0.44466403]\n",
      "  [0.91501975 0.7509882  0.44071147]\n",
      "  [0.91106725 0.7490118  0.44466403]]\n",
      "\n",
      " [[0.64031625 0.40711462 0.4110672 ]\n",
      "  [0.6442688  0.4090909  0.41501978]\n",
      "  [0.6620553  0.40711462 0.41501978]\n",
      "  ...\n",
      "  [0.9071146  0.75494075 0.4367589 ]\n",
      "  [0.9071146  0.80830044 0.44071147]\n",
      "  [0.91106725 0.81225294 0.4525692 ]]\n",
      "\n",
      " [[0.64624506 0.41304347 0.41304347]\n",
      "  [0.64624506 0.4110672  0.41699606]\n",
      "  [0.6739131  0.4110672  0.41699606]\n",
      "  ...\n",
      "  [0.9249012  0.77470356 0.45454547]\n",
      "  [0.91106725 0.81422925 0.4466403 ]\n",
      "  [0.91501975 0.8181818  0.4525692 ]]]\n",
      "[[[0.65217394 0.43083003 0.44071147]\n",
      "  [0.65217394 0.43478262 0.44071147]\n",
      "  [0.6482214  0.43280634 0.4367589 ]\n",
      "  ...\n",
      "  [0.56521744 0.24901186 0.3339921 ]\n",
      "  [0.5335969  0.22529644 0.32015812]\n",
      "  [0.5237155  0.19169961 0.30632412]]\n",
      "\n",
      " [[0.66007906 0.4466403  0.45059288]\n",
      "  [0.66007906 0.4387352  0.4387352 ]\n",
      "  [0.66600794 0.44071147 0.4387352 ]\n",
      "  ...\n",
      "  [0.5513834  0.24110672 0.33003953]\n",
      "  [0.54347825 0.22727273 0.31422925]\n",
      "  [0.53952575 0.23913044 0.32015812]]\n",
      "\n",
      " [[0.64229256 0.4367589  0.44071147]\n",
      "  [0.6541502  0.4367589  0.4387352 ]\n",
      "  [0.66007906 0.4367589  0.44071147]\n",
      "  ...\n",
      "  [0.5889328  0.22924902 0.3221344 ]\n",
      "  [0.63438743 0.28063244 0.33596838]\n",
      "  [0.6620553  0.34980237 0.39130434]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.64624506 0.4110672  0.41699606]\n",
      "  [0.63833994 0.4090909  0.41304347]\n",
      "  [0.63438743 0.40316206 0.4110672 ]\n",
      "  ...\n",
      "  [0.6086957  0.9525692  0.7490118 ]\n",
      "  [0.4624506  0.95454544 0.89525694]\n",
      "  [0.45849803 0.9525692  0.89920944]]\n",
      "\n",
      " [[0.64031625 0.40711462 0.4110672 ]\n",
      "  [0.6442688  0.4090909  0.41501978]\n",
      "  [0.63833994 0.40711462 0.41501978]\n",
      "  ...\n",
      "  [0.45454547 0.9426878  0.8913043 ]\n",
      "  [0.40316206 0.8241107  0.94466406]\n",
      "  [0.40711462 0.82806325 0.95652175]]\n",
      "\n",
      " [[0.64624506 0.41304347 0.41304347]\n",
      "  [0.64624506 0.4110672  0.41699606]\n",
      "  [0.6501977  0.4110672  0.41699606]\n",
      "  ...\n",
      "  [0.47233203 0.96245056 0.90909094]\n",
      "  [0.40711462 0.83003956 0.95059294]\n",
      "  [0.4110672  0.8339921  0.95652175]]]\n",
      "[[[0.6653226  0.43951613 0.44959676]\n",
      "  [0.6653226  0.44354838 0.44959676]\n",
      "  [0.66129035 0.44153225 0.4455645 ]\n",
      "  ...\n",
      "  [0.57661295 0.25403225 0.3407258 ]\n",
      "  [0.54435486 0.22983871 0.32661292]\n",
      "  [0.5342743  0.19556452 0.3125    ]]\n",
      "\n",
      " [[0.6733871  0.45564517 0.45967743]\n",
      "  [0.6733871  0.44758064 0.44758064]\n",
      "  [0.67943555 0.44959676 0.44758064]\n",
      "  ...\n",
      "  [0.5625     0.24596775 0.33669356]\n",
      "  [0.5544355  0.23185484 0.32056454]\n",
      "  [0.5504033  0.24395162 0.32661292]]\n",
      "\n",
      " [[0.65524197 0.4455645  0.44959676]\n",
      "  [0.6673387  0.4455645  0.44758064]\n",
      "  [0.6733871  0.4455645  0.44959676]\n",
      "  ...\n",
      "  [0.5604839  0.23387097 0.32862905]\n",
      "  [0.5987904  0.28629035 0.34274194]\n",
      "  [0.6270161  0.35685486 0.39919356]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.65927416 0.41935486 0.42540324]\n",
      "  [0.65120965 0.41733873 0.42137098]\n",
      "  [0.64717746 0.41129032 0.41935486]\n",
      "  ...\n",
      "  [0.6733871  0.4576613  0.45362905]\n",
      "  [0.67741936 0.45967743 0.44959676]\n",
      "  [0.6733871  0.4576613  0.45362905]]\n",
      "\n",
      " [[0.65322584 0.41532257 0.41935486]\n",
      "  [0.6572581  0.41733873 0.4233871 ]\n",
      "  [0.65120965 0.41532257 0.4233871 ]\n",
      "  ...\n",
      "  [0.66935486 0.44758064 0.4455645 ]\n",
      "  [0.66935486 0.45362905 0.44959676]\n",
      "  [0.6733871  0.4576613  0.46169356]]\n",
      "\n",
      " [[0.65927416 0.42137098 0.42137098]\n",
      "  [0.65927416 0.41935486 0.42540324]\n",
      "  [0.6633065  0.41935486 0.42540324]\n",
      "  ...\n",
      "  [0.68750006 0.46774194 0.46370968]\n",
      "  [0.6733871  0.45967743 0.45564517]\n",
      "  [0.67741936 0.46370968 0.46169356]]]\n",
      "[[[0.65346533 0.43168315 0.44158414]\n",
      "  [0.65346533 0.43564355 0.44158414]\n",
      "  [0.64950496 0.43366337 0.43762374]\n",
      "  ...\n",
      "  [0.56633663 0.24950494 0.33465347]\n",
      "  [0.5346535  0.22574256 0.32079208]\n",
      "  [0.5247525  0.1920792  0.3069307 ]]\n",
      "\n",
      " [[0.66138613 0.44752476 0.45148513]\n",
      "  [0.66138613 0.43960395 0.43960395]\n",
      "  [0.66732675 0.44158414 0.43960395]\n",
      "  ...\n",
      "  [0.5524752  0.24158415 0.33069307]\n",
      "  [0.5445544  0.22772276 0.3148515 ]\n",
      "  [0.5405941  0.23960395 0.32079208]]\n",
      "\n",
      " [[0.6435644  0.43762374 0.44158414]\n",
      "  [0.6554455  0.43762374 0.43960395]\n",
      "  [0.66138613 0.43762374 0.44158414]\n",
      "  ...\n",
      "  [0.5504951  0.22970296 0.32277226]\n",
      "  [0.58811885 0.28118813 0.33663365]\n",
      "  [0.61584157 0.35049504 0.3920792 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6475247  0.41188118 0.41782176]\n",
      "  [0.6396039  0.409901   0.41386136]\n",
      "  [0.6356436  0.40396038 0.41188118]\n",
      "  ...\n",
      "  [0.66138613 0.44950494 0.44554454]\n",
      "  [0.66534656 0.45148513 0.44158414]\n",
      "  [0.66138613 0.44950494 0.44554454]]\n",
      "\n",
      " [[0.64158416 0.40792078 0.41188118]\n",
      "  [0.6455445  0.409901   0.41584158]\n",
      "  [0.6396039  0.40792078 0.41584158]\n",
      "  ...\n",
      "  [0.65742576 0.43960395 0.43762374]\n",
      "  [0.65742576 0.44554454 0.44158414]\n",
      "  [0.66138613 0.44950494 0.45346534]]\n",
      "\n",
      " [[0.6475247  0.41386136 0.41386136]\n",
      "  [0.6475247  0.41188118 0.41782176]\n",
      "  [0.6514852  0.41188118 0.41782176]\n",
      "  ...\n",
      "  [0.67524755 0.45940593 0.45544553]\n",
      "  [0.66138613 0.45148513 0.44752476]\n",
      "  [0.66534656 0.45544553 0.45346534]]]\n",
      "[[[0.70512825 0.46581197 0.47649574]\n",
      "  [0.70512825 0.47008547 0.47649574]\n",
      "  [0.7008547  0.46794873 0.47222224]\n",
      "  ...\n",
      "  [0.61111116 0.26923075 0.36111113]\n",
      "  [0.57692313 0.24358974 0.34615386]\n",
      "  [0.56623936 0.20726496 0.3311966 ]]\n",
      "\n",
      " [[0.71367526 0.48290598 0.4871795 ]\n",
      "  [0.71367526 0.47435898 0.47435898]\n",
      "  [0.7200855  0.47649574 0.47435898]\n",
      "  ...\n",
      "  [0.59615386 0.26068375 0.35683763]\n",
      "  [0.58760685 0.2457265  0.3397436 ]\n",
      "  [0.5833334  0.258547   0.34615386]]\n",
      "\n",
      " [[0.6944445  0.47222224 0.47649574]\n",
      "  [0.70726496 0.47222224 0.47435898]\n",
      "  [0.71367526 0.47222224 0.47649574]\n",
      "  ...\n",
      "  [0.59401715 0.24786325 0.34829062]\n",
      "  [0.6346154  0.30341882 0.36324787]\n",
      "  [0.6645299  0.37820515 0.42307693]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.69871795 0.44444445 0.45085472]\n",
      "  [0.69017094 0.4423077  0.4465812 ]\n",
      "  [0.68589747 0.43589744 0.44444445]\n",
      "  ...\n",
      "  [0.71367526 0.48504275 0.48076925]\n",
      "  [0.71794873 0.4871795  0.47649574]\n",
      "  [0.71367526 0.48504275 0.48076925]]\n",
      "\n",
      " [[0.6923077  0.44017094 0.44444445]\n",
      "  [0.69658124 0.4423077  0.44871795]\n",
      "  [0.69017094 0.44017094 0.44871795]\n",
      "  ...\n",
      "  [0.7094017  0.47435898 0.47222224]\n",
      "  [0.7094017  0.48076925 0.47649574]\n",
      "  [0.71367526 0.48504275 0.48931625]]\n",
      "\n",
      " [[0.69871795 0.4465812  0.4465812 ]\n",
      "  [0.69871795 0.44444445 0.45085472]\n",
      "  [0.7029915  0.44444445 0.45085472]\n",
      "  ...\n",
      "  [0.7286325  0.4957265  0.491453  ]\n",
      "  [0.71367526 0.4871795  0.48290598]\n",
      "  [0.71794873 0.491453   0.48931625]]]\n",
      "[[[0.65868264 0.43512973 0.44510978]\n",
      "  [0.65868264 0.43912175 0.44510978]\n",
      "  [0.6546906  0.43712574 0.44111776]\n",
      "  ...\n",
      "  [0.5708583  0.251497   0.33732533]\n",
      "  [0.5389222  0.2275449  0.3233533 ]\n",
      "  [0.52894217 0.19361277 0.30938125]]\n",
      "\n",
      " [[0.6666667  0.4510978  0.4550898 ]\n",
      "  [0.6666667  0.44311377 0.44311377]\n",
      "  [0.67265475 0.44510978 0.44311377]\n",
      "  ...\n",
      "  [0.5568862  0.24351297 0.33333334]\n",
      "  [0.54890215 0.22954091 0.31736526]\n",
      "  [0.5449102  0.24151696 0.3233533 ]]\n",
      "\n",
      " [[0.6487026  0.44111776 0.44510978]\n",
      "  [0.6606786  0.44111776 0.44311377]\n",
      "  [0.6666667  0.44111776 0.44510978]\n",
      "  ...\n",
      "  [0.5548902  0.23153692 0.3253493 ]\n",
      "  [0.5928144  0.28343314 0.33932135]\n",
      "  [0.6207585  0.35329342 0.39520958]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6526946  0.41516966 0.4211577 ]\n",
      "  [0.64471054 0.41317365 0.41716567]\n",
      "  [0.6407186  0.4071856  0.41516966]\n",
      "  ...\n",
      "  [0.6666667  0.4530938  0.44910178]\n",
      "  [0.6706587  0.4550898  0.44510978]\n",
      "  [0.6666667  0.4530938  0.44910178]]\n",
      "\n",
      " [[0.6467066  0.41117764 0.41516966]\n",
      "  [0.6506986  0.41317365 0.41916168]\n",
      "  [0.64471054 0.41117764 0.41916168]\n",
      "  ...\n",
      "  [0.66267467 0.44311377 0.44111776]\n",
      "  [0.66267467 0.44910178 0.44510978]\n",
      "  [0.6666667  0.4530938  0.45708582]]\n",
      "\n",
      " [[0.6526946  0.41716567 0.41716567]\n",
      "  [0.6526946  0.41516966 0.4211577 ]\n",
      "  [0.65668666 0.41516966 0.4211577 ]\n",
      "  ...\n",
      "  [0.68063873 0.46307385 0.45908183]\n",
      "  [0.6666667  0.4550898  0.4510978 ]\n",
      "  [0.6706587  0.45908183 0.45708582]]]\n",
      "[[[0.6962026  0.4599156  0.47046414]\n",
      "  [0.6962026  0.46413502 0.47046414]\n",
      "  [0.69198316 0.4620253  0.46624473]\n",
      "  ...\n",
      "  [0.60337555 0.2658228  0.35654008]\n",
      "  [0.56962025 0.24050634 0.34177217]\n",
      "  [0.5590718  0.20464136 0.32700422]]\n",
      "\n",
      " [[0.70464134 0.47679326 0.48101267]\n",
      "  [0.70464134 0.46835443 0.46835443]\n",
      "  [0.7109705  0.47046414 0.46835443]\n",
      "  ...\n",
      "  [0.5886076  0.25738397 0.35232067]\n",
      "  [0.5801688  0.24261603 0.33544305]\n",
      "  [0.57594943 0.25527427 0.34177217]]\n",
      "\n",
      " [[0.68565404 0.46624473 0.47046414]\n",
      "  [0.6983122  0.46624473 0.46835443]\n",
      "  [0.70464134 0.46624473 0.47046414]\n",
      "  ...\n",
      "  [0.5864979  0.24472573 0.34388188]\n",
      "  [0.6265823  0.29957807 0.3586498 ]\n",
      "  [0.65611815 0.37341774 0.4177215 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6898734  0.43881857 0.4451477 ]\n",
      "  [0.6814346  0.43670887 0.44092828]\n",
      "  [0.6772152  0.43037975 0.43881857]\n",
      "  ...\n",
      "  [0.70464134 0.47890297 0.47468355]\n",
      "  [0.70886075 0.48101267 0.47046414]\n",
      "  [0.70464134 0.47890297 0.47468355]]\n",
      "\n",
      " [[0.68354434 0.43459916 0.43881857]\n",
      "  [0.68776375 0.43670887 0.443038  ]\n",
      "  [0.6814346  0.43459916 0.443038  ]\n",
      "  ...\n",
      "  [0.700422   0.46835443 0.46624473]\n",
      "  [0.700422   0.47468355 0.47046414]\n",
      "  [0.70464134 0.47890297 0.48312238]]\n",
      "\n",
      " [[0.6898734  0.44092828 0.44092828]\n",
      "  [0.6898734  0.43881857 0.4451477 ]\n",
      "  [0.69409287 0.43881857 0.4451477 ]\n",
      "  ...\n",
      "  [0.71940935 0.48945147 0.48523206]\n",
      "  [0.70464134 0.48101267 0.47679326]\n",
      "  [0.70886075 0.48523206 0.48312238]]]\n",
      "[[[0.6547619  0.4325397  0.44246033]\n",
      "  [0.6547619  0.43650794 0.44246033]\n",
      "  [0.6507937  0.43452382 0.43849206]\n",
      "  ...\n",
      "  [0.56746036 0.25       0.33531746]\n",
      "  [0.5357143  0.22619048 0.3214286 ]\n",
      "  [0.52579373 0.19246033 0.3075397 ]]\n",
      "\n",
      " [[0.66269845 0.44841272 0.45238096]\n",
      "  [0.66269845 0.4404762  0.4404762 ]\n",
      "  [0.66865087 0.44246033 0.4404762 ]\n",
      "  ...\n",
      "  [0.5535714  0.24206349 0.33134922]\n",
      "  [0.5456349  0.22817461 0.3154762 ]\n",
      "  [0.54166675 0.24007937 0.3214286 ]]\n",
      "\n",
      " [[0.6448413  0.43849206 0.44246033]\n",
      "  [0.65674603 0.43849206 0.4404762 ]\n",
      "  [0.66269845 0.43849206 0.44246033]\n",
      "  ...\n",
      "  [0.55158734 0.23015873 0.32341272]\n",
      "  [0.5892858  0.28174606 0.3373016 ]\n",
      "  [0.61706346 0.35119048 0.39285716]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6488095  0.41269842 0.4186508 ]\n",
      "  [0.640873   0.4107143  0.41468254]\n",
      "  [0.63690484 0.4047619  0.41269842]\n",
      "  ...\n",
      "  [0.66269845 0.45039684 0.44642857]\n",
      "  [0.6666667  0.45238096 0.44246033]\n",
      "  [0.66269845 0.45039684 0.44642857]]\n",
      "\n",
      " [[0.6428572  0.40873018 0.41269842]\n",
      "  [0.64682543 0.4107143  0.4166667 ]\n",
      "  [0.640873   0.40873018 0.4166667 ]\n",
      "  ...\n",
      "  [0.6587302  0.4404762  0.43849206]\n",
      "  [0.6587302  0.44642857 0.44246033]\n",
      "  [0.66269845 0.45039684 0.45436507]]\n",
      "\n",
      " [[0.6488095  0.41468254 0.41468254]\n",
      "  [0.6488095  0.41269842 0.4186508 ]\n",
      "  [0.65277785 0.41269842 0.4186508 ]\n",
      "  ...\n",
      "  [0.67658734 0.46031746 0.45634922]\n",
      "  [0.66269845 0.45238096 0.44841272]\n",
      "  [0.6666667  0.45634922 0.45436507]]]\n",
      "[[[0.66       0.436      0.446     ]\n",
      "  [0.66       0.44       0.446     ]\n",
      "  [0.656      0.438      0.442     ]\n",
      "  ...\n",
      "  [0.716      0.252      0.338     ]\n",
      "  [0.684      0.228      0.324     ]\n",
      "  [0.674      0.194      0.31      ]]\n",
      "\n",
      " [[0.66800004 0.452      0.456     ]\n",
      "  [0.66800004 0.444      0.444     ]\n",
      "  [0.674      0.446      0.444     ]\n",
      "  ...\n",
      "  [0.70199996 0.244      0.33400002]\n",
      "  [0.694      0.23       0.31800002]\n",
      "  [0.69000006 0.242      0.324     ]]\n",
      "\n",
      " [[0.65000004 0.442      0.446     ]\n",
      "  [0.662      0.442      0.444     ]\n",
      "  [0.66800004 0.442      0.446     ]\n",
      "  ...\n",
      "  [0.70000005 0.23200001 0.326     ]\n",
      "  [0.73800004 0.284      0.34      ]\n",
      "  [0.766      0.354      0.396     ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.654      0.416      0.422     ]\n",
      "  [0.64599997 0.414      0.418     ]\n",
      "  [0.6420001  0.408      0.416     ]\n",
      "  ...\n",
      "  [0.66800004 0.454      0.45000002]\n",
      "  [0.672      0.456      0.446     ]\n",
      "  [0.66800004 0.454      0.45000002]]\n",
      "\n",
      " [[0.648      0.412      0.416     ]\n",
      "  [0.652      0.414      0.42000002]\n",
      "  [0.64599997 0.412      0.42000002]\n",
      "  ...\n",
      "  [0.66400003 0.444      0.442     ]\n",
      "  [0.66400003 0.45000002 0.446     ]\n",
      "  [0.66800004 0.454      0.458     ]]\n",
      "\n",
      " [[0.654      0.418      0.418     ]\n",
      "  [0.654      0.416      0.422     ]\n",
      "  [0.65800005 0.416      0.422     ]\n",
      "  ...\n",
      "  [0.68200004 0.46400002 0.46      ]\n",
      "  [0.66800004 0.456      0.452     ]\n",
      "  [0.672      0.46       0.458     ]]]\n",
      "[[[0.66       0.436      0.446     ]\n",
      "  [0.66       0.44       0.446     ]\n",
      "  [0.656      0.438      0.442     ]\n",
      "  ...\n",
      "  [0.572      0.252      0.338     ]\n",
      "  [0.54       0.228      0.324     ]\n",
      "  [0.53000003 0.194      0.31      ]]\n",
      "\n",
      " [[0.66800004 0.452      0.456     ]\n",
      "  [0.66800004 0.444      0.444     ]\n",
      "  [0.674      0.446      0.444     ]\n",
      "  ...\n",
      "  [0.55799997 0.244      0.33400002]\n",
      "  [0.55       0.23       0.31800002]\n",
      "  [0.54600006 0.242      0.324     ]]\n",
      "\n",
      " [[0.65000004 0.442      0.446     ]\n",
      "  [0.662      0.442      0.444     ]\n",
      "  [0.66800004 0.442      0.446     ]\n",
      "  ...\n",
      "  [0.55600005 0.23200001 0.326     ]\n",
      "  [0.59400004 0.284      0.34      ]\n",
      "  [0.622      0.354      0.396     ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.654      0.416      0.422     ]\n",
      "  [0.64599997 0.414      0.418     ]\n",
      "  [0.6420001  0.408      0.416     ]\n",
      "  ...\n",
      "  [0.66800004 0.454      0.45000002]\n",
      "  [0.672      0.456      0.446     ]\n",
      "  [0.66800004 0.454      0.45000002]]\n",
      "\n",
      " [[0.648      0.412      0.416     ]\n",
      "  [0.652      0.414      0.42000002]\n",
      "  [0.64599997 0.412      0.42000002]\n",
      "  ...\n",
      "  [0.66400003 0.444      0.442     ]\n",
      "  [0.66400003 0.45000002 0.446     ]\n",
      "  [0.66800004 0.454      0.458     ]]\n",
      "\n",
      " [[0.654      0.418      0.418     ]\n",
      "  [0.654      0.416      0.422     ]\n",
      "  [0.65800005 0.416      0.422     ]\n",
      "  ...\n",
      "  [0.68200004 0.46400002 0.46      ]\n",
      "  [0.66800004 0.456      0.452     ]\n",
      "  [0.672      0.46       0.458     ]]]\n",
      "[[[0.61538464 0.93293893 0.72781074]\n",
      "  [0.61538464 0.9368836  0.72781074]\n",
      "  [0.6114399  0.93491125 0.72386587]\n",
      "  ...\n",
      "  [0.5680474  0.75147927 0.58185405]\n",
      "  [0.5364892  0.7278107  0.5680474 ]\n",
      "  [0.52662724 0.69428015 0.55424064]]\n",
      "\n",
      " [[0.6232742  0.948718   0.7376726 ]\n",
      "  [0.6232742  0.94082844 0.7258383 ]\n",
      "  [0.62919134 0.9428008  0.7258383 ]\n",
      "  ...\n",
      "  [0.5542407  0.74358976 0.5779093 ]\n",
      "  [0.54635113 0.72978306 0.5621302 ]\n",
      "  [0.5424063  0.7416174  0.5680474 ]]\n",
      "\n",
      " [[0.6055227  0.93885607 0.72781074]\n",
      "  [0.617357   0.93885607 0.7258383 ]\n",
      "  [0.6232742  0.93885607 0.72781074]\n",
      "  ...\n",
      "  [0.55226827 0.73175544 0.5700197 ]\n",
      "  [0.5897436  0.78303754 0.5838265 ]\n",
      "  [0.6173571  0.85207105 0.6390533 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.8954635  0.64694285 0.41617358]\n",
      "  [0.887574   0.6449704  0.41222882]\n",
      "  [0.8836292  0.6390533  0.41025642]\n",
      "  ...\n",
      "  [0.40631166 0.5424063  0.9467456 ]\n",
      "  [0.41025642 0.54437876 0.9428008 ]\n",
      "  [0.40631166 0.5424063  0.9467456 ]]\n",
      "\n",
      " [[0.88954633 0.64299804 0.41025642]\n",
      "  [0.8934912  0.6449704  0.4142012 ]\n",
      "  [0.887574   0.64299804 0.4142012 ]\n",
      "  ...\n",
      "  [0.40236688 0.53254443 0.93885607]\n",
      "  [0.40236688 0.53846157 0.9428008 ]\n",
      "  [0.40631166 0.5424063  0.95463514]]\n",
      "\n",
      " [[0.8954635  0.6489152  0.41222882]\n",
      "  [0.8954635  0.64694285 0.41617358]\n",
      "  [0.89940834 0.64694285 0.41617358]\n",
      "  ...\n",
      "  [0.42011836 0.55226827 0.9566076 ]\n",
      "  [0.40631166 0.54437876 0.948718  ]\n",
      "  [0.41025642 0.5483235  0.95463514]]]\n",
      "[[[0.3992095  0.88142294 0.94466406]\n",
      "  [0.3992095  0.8853755  0.94466406]\n",
      "  [0.39525694 0.8833992  0.94071144]\n",
      "  ...\n",
      "  [0.3478261  0.75296444 0.8043478 ]\n",
      "  [0.31620553 0.729249   0.7905139 ]\n",
      "  [0.30632412 0.6956522  0.7766799 ]]\n",
      "\n",
      " [[0.40711462 0.8972332  0.95454544]\n",
      "  [0.40711462 0.88932806 0.9426878 ]\n",
      "  [0.41304347 0.8913043  0.9426878 ]\n",
      "  ...\n",
      "  [0.3339921  0.74505925 0.8003953 ]\n",
      "  [0.32608697 0.7312253  0.784585  ]\n",
      "  [0.3221344  0.743083   0.7905139 ]]\n",
      "\n",
      " [[0.38932806 0.8873518  0.94466406]\n",
      "  [0.40118578 0.8873518  0.9426878 ]\n",
      "  [0.40711462 0.8873518  0.94466406]\n",
      "  ...\n",
      "  [0.3320158  0.73320156 0.7924902 ]\n",
      "  [0.36956522 0.784585   0.8063241 ]\n",
      "  [0.39723322 0.85375494 0.86166006]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.8972332  0.6166008  0.41699606]\n",
      "  [0.88932806 0.6146245  0.41304347]\n",
      "  [0.8853755  0.6086957  0.4110672 ]\n",
      "  ...\n",
      "  [0.40711462 0.4486166  0.8636364 ]\n",
      "  [0.4110672  0.45059288 0.85968375]\n",
      "  [0.40711462 0.4486166  0.8636364 ]]\n",
      "\n",
      " [[0.8913043  0.61264825 0.4110672 ]\n",
      "  [0.89525694 0.6146245  0.41501978]\n",
      "  [0.88932806 0.61264825 0.41501978]\n",
      "  ...\n",
      "  [0.40316206 0.4387352  0.85573125]\n",
      "  [0.40316206 0.44466403 0.85968375]\n",
      "  [0.40711462 0.4486166  0.87154156]]\n",
      "\n",
      " [[0.8972332  0.61857706 0.41304347]\n",
      "  [0.8972332  0.6166008  0.41699606]\n",
      "  [0.90118575 0.6166008  0.41699606]\n",
      "  ...\n",
      "  [0.42094862 0.45849803 0.8735178 ]\n",
      "  [0.40711462 0.45059288 0.8656127 ]\n",
      "  [0.4110672  0.45454547 0.87154156]]]\n",
      "[[[0.39842212 0.4299803  0.82642996]\n",
      "  [0.39842212 0.43392506 0.82642996]\n",
      "  [0.39447734 0.4319527  0.82248527]\n",
      "  ...\n",
      "  [0.41025642 0.75147927 0.739645  ]\n",
      "  [0.37869823 0.7278107  0.7258383 ]\n",
      "  [0.3688363  0.69428015 0.7120316 ]]\n",
      "\n",
      " [[0.40631166 0.4457594  0.83629197]\n",
      "  [0.40631166 0.43786985 0.82445765]\n",
      "  [0.41222882 0.43984222 0.82445765]\n",
      "  ...\n",
      "  [0.3964497  0.74358976 0.73570025]\n",
      "  [0.38856018 0.72978306 0.7199212 ]\n",
      "  [0.3846154  0.7416174  0.7258383 ]]\n",
      "\n",
      " [[0.38856018 0.43589744 0.82642996]\n",
      "  [0.4003945  0.43589744 0.82445765]\n",
      "  [0.40631166 0.43589744 0.82642996]\n",
      "  ...\n",
      "  [0.39447734 0.73175544 0.72781074]\n",
      "  [0.4319527  0.78303754 0.7416174 ]\n",
      "  [0.4595661  0.85207105 0.79684424]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.8954635  0.7731756  0.41617358]\n",
      "  [0.887574   0.77120316 0.41222882]\n",
      "  [0.8836292  0.765286   0.41025642]\n",
      "  ...\n",
      "  [0.40631166 0.7080868  0.9467456 ]\n",
      "  [0.41025642 0.7100592  0.9428008 ]\n",
      "  [0.40631166 0.7080868  0.9467456 ]]\n",
      "\n",
      " [[0.88954633 0.7692308  0.41025642]\n",
      "  [0.8934912  0.77120316 0.4142012 ]\n",
      "  [0.887574   0.7692308  0.4142012 ]\n",
      "  ...\n",
      "  [0.40236688 0.6982249  0.93885607]\n",
      "  [0.40236688 0.7041421  0.9428008 ]\n",
      "  [0.40631166 0.7080868  0.95463514]]\n",
      "\n",
      " [[0.8954635  0.775148   0.41222882]\n",
      "  [0.8954635  0.7731756  0.41617358]\n",
      "  [0.89940834 0.7731756  0.41617358]\n",
      "  ...\n",
      "  [0.42011836 0.71794873 0.9566076 ]\n",
      "  [0.40631166 0.7100592  0.948718  ]\n",
      "  [0.41025642 0.714004   0.95463514]]]\n",
      "[[[0.7258383  0.93293893 0.617357  ]\n",
      "  [0.7258383  0.9368836  0.617357  ]\n",
      "  [0.72189355 0.93491125 0.61341226]\n",
      "  ...\n",
      "  [0.8145957  0.37475348 0.33333334]\n",
      "  [0.78303754 0.35108483 0.31952664]\n",
      "  [0.7731756  0.31755427 0.30571994]]\n",
      "\n",
      " [[0.7337279  0.948718   0.62721896]\n",
      "  [0.7337279  0.94082844 0.61538464]\n",
      "  [0.739645   0.9428008  0.61538464]\n",
      "  ...\n",
      "  [0.800789   0.36686394 0.3293886 ]\n",
      "  [0.7928994  0.3530572  0.31360948]\n",
      "  [0.7889547  0.36489153 0.31952664]]\n",
      "\n",
      " [[0.71597636 0.93885607 0.617357  ]\n",
      "  [0.72781074 0.93885607 0.61538464]\n",
      "  [0.7337279  0.93885607 0.617357  ]\n",
      "  ...\n",
      "  [0.7988166  0.3550296  0.32149902]\n",
      "  [0.83629197 0.4063117  0.33530575]\n",
      "  [0.8639054  0.4753452  0.39053255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.6449704  0.41025642 0.41617358]\n",
      "  [0.6370809  0.40828404 0.41222882]\n",
      "  [0.63313615 0.40236688 0.41025642]\n",
      "  ...\n",
      "  [0.9092703  0.92110455 0.443787  ]\n",
      "  [0.913215   0.9230769  0.43984222]\n",
      "  [0.9092703  0.92110455 0.443787  ]]\n",
      "\n",
      " [[0.6390533  0.40631166 0.41025642]\n",
      "  [0.64299804 0.40828404 0.4142012 ]\n",
      "  [0.6370809  0.40631166 0.4142012 ]\n",
      "  ...\n",
      "  [0.9053255  0.91124266 0.43589744]\n",
      "  [0.9053255  0.91715986 0.43984222]\n",
      "  [0.9092703  0.92110455 0.45167655]]\n",
      "\n",
      " [[0.6449704  0.41222882 0.41222882]\n",
      "  [0.6449704  0.41025642 0.41617358]\n",
      "  [0.64891523 0.41025642 0.41617358]\n",
      "  ...\n",
      "  [0.9230769  0.9309665  0.45364892]\n",
      "  [0.9092703  0.9230769  0.4457594 ]\n",
      "  [0.913215   0.92702174 0.45167655]]]\n",
      "[[[0.9049505  0.74851483 0.44158414]\n",
      "  [0.9049505  0.75247526 0.44158414]\n",
      "  [0.90099007 0.750495   0.43762374]\n",
      "  ...\n",
      "  [0.6455445  0.24950494 0.33465347]\n",
      "  [0.6138614  0.22574256 0.32079208]\n",
      "  [0.60396045 0.1920792  0.3069307 ]]\n",
      "\n",
      " [[0.9128713  0.76435643 0.45148513]\n",
      "  [0.9128713  0.75643563 0.43960395]\n",
      "  [0.91881186 0.7584158  0.43960395]\n",
      "  ...\n",
      "  [0.6316832  0.24158415 0.33069307]\n",
      "  [0.62376237 0.22772276 0.3148515 ]\n",
      "  [0.619802   0.23960395 0.32079208]]\n",
      "\n",
      " [[0.8950495  0.75445545 0.44158414]\n",
      "  [0.9069307  0.75445545 0.43960395]\n",
      "  [0.9128713  0.75445545 0.44158414]\n",
      "  ...\n",
      "  [0.629703   0.22970296 0.32277226]\n",
      "  [0.66732675 0.28118813 0.33663365]\n",
      "  [0.69504946 0.35049504 0.3920792 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.71089107 0.41188118 0.41782176]\n",
      "  [0.70297027 0.409901   0.41386136]\n",
      "  [0.69900995 0.40396038 0.41188118]\n",
      "  ...\n",
      "  [0.9128713  0.44950494 0.44554454]\n",
      "  [0.9168316  0.45148513 0.44158414]\n",
      "  [0.9128713  0.44950494 0.44554454]]\n",
      "\n",
      " [[0.7049505  0.40792078 0.41188118]\n",
      "  [0.7089109  0.409901   0.41584158]\n",
      "  [0.70297027 0.40792078 0.41584158]\n",
      "  ...\n",
      "  [0.9089108  0.43960395 0.43762374]\n",
      "  [0.9089108  0.44554454 0.44158414]\n",
      "  [0.9128713  0.44950494 0.45346534]]\n",
      "\n",
      " [[0.71089107 0.41386136 0.41386136]\n",
      "  [0.71089107 0.41188118 0.41782176]\n",
      "  [0.7148515  0.41188118 0.41782176]\n",
      "  ...\n",
      "  [0.92673266 0.45940593 0.45544553]\n",
      "  [0.9128713  0.45148513 0.44752476]\n",
      "  [0.9168316  0.45544553 0.45346534]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hiper_model = load_model_weights(Net(net_version=\"b0\", num_classes=2).to('cpu'), hiper_path)\n",
    "\n",
    "grad_cam = GradCam(model=hiper_model, blob_name = '_blocks', target_layer_names=[str(i) for i in range(1,16)], use_cuda=False)\n",
    "img = cv2.imread('../data/raw/hipercellularity/PAS/PSHIPERCELULARIDADE20200802-1394.jpg', 1)\n",
    "img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "inputs = preprocess_image(img)\n",
    "# If None, returns the map for the highest scoring category.\n",
    "# Otherwise, targets the requested index.\n",
    "target_index = None\n",
    "mask_dic = grad_cam(inputs, target_index)\n",
    "show_cams(img, mask_dic)\n",
    "gb_model = GuidedBackpropReLUModel(model=hiper_model, activation_layer_name = 'MemoryEfficientSwish', use_cuda=False)\n",
    "show_gbs(inputs, gb_model, target_index, mask_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\terumo-model-binary-glomerulus-hypercellularity\\notebooks\\06_explanability.ipynb Cell 9\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_gradient\u001b[39m( grad):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     gradients\u001b[39m.\u001b[39mappend(grad)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m inputs\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m blob_name \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_blocks\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, module \u001b[39min\u001b[39;00m hiper_model\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "gradients =[]\n",
    "outputs = []\n",
    "def save_gradient( grad):\n",
    "    gradients.append(grad)\n",
    "x = inputs\n",
    "blob_name ='_blocks'\n",
    "\n",
    "for idx, module in hiper_model._modules.items():\n",
    "        if idx != blob_name:\n",
    "            try:\n",
    "                x = module(x)\n",
    "            except:\n",
    "                x = x.view(x.size(0), -1)\n",
    "                x = module(x)\n",
    "        else:\n",
    "            for name, block in enumerate(getattr(hiper_model,blob_name)):\n",
    "                x = block(x)\n",
    "                if str(name) in ['1','10','15']:\n",
    "                    x.register_hook(save_gradient)\n",
    "                    outputs += [x] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute '_blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\terumo-model-binary-glomerulus-hypercellularity\\notebooks\\06_explanability.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mgetattr\u001b[39;49m(hiper_model,blob_name)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(block)\n",
      "File \u001b[1;32mc:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Net' object has no attribute '_blocks'"
     ]
    }
   ],
   "source": [
    "for name, block in enumerate(getattr(hiper_model,blob_name)):\n",
    "    print(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'EfficientNet' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Maods\\Documents\\Development\\Mestrado\\terumo\\apps\\terumo-model-binary-glomerulus-hypercellularity\\notebooks\\06_explanability.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hiper_model\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Maods/Documents/Development/Mestrado/terumo/apps/terumo-model-binary-glomerulus-hypercellularity/notebooks/06_explanability.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m hiper_model\u001b[39m.\u001b[39;49mbackbone[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'EfficientNet' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "hiper_model._modules.items()\n",
    "hiper_model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning:\n",
      "\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_clustering.py:35: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_clustering.py:54: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_clustering.py:63: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_clustering.py:69: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_clustering.py:77: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\links.py:5: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\links.py:10: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\links.py:15: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\links.py:20: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_masked_model.py:363: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_masked_model.py:385: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_masked_model.py:428: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\utils\\_masked_model.py:439: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\maskers\\_tabular.py:186: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\maskers\\_tabular.py:197: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\maskers\\_image.py:175: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "c:\\Users\\Maods\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\shap\\explainers\\_partition.py:676: NumbaDeprecationWarning:\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image as PilImage\n",
    "\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.explainers.vision.specific.gradcam.pytorch.gradcam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image(PilImage.open('../data/raw/hipercellularity/PAS/2003PC0154af.jpg').convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    }
   ],
   "source": [
    "explainer = GradCAM(\n",
    "    model=model,\n",
    "    target_layer=model._blocks[4],\n",
    "    preprocess_function=preprocess\n",
    ")\n",
    "# Explain the top label\n",
    "explanations = explainer.explain(img)\n",
    "explanations.ipython_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBConvBlock(\n",
       "  (_expand_conv): Conv2dStaticSamePadding(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "    96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_se_reduce): Conv2dStaticSamePadding(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_se_expand): Conv2dStaticSamePadding(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_project_conv): Conv2dStaticSamePadding(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiper_model.backbone._blocks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('backbone',\n",
       "               EfficientNet(\n",
       "                 (_conv_stem): Conv2dStaticSamePadding(\n",
       "                   3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "                   (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "                 )\n",
       "                 (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                 (_blocks): ModuleList(\n",
       "                   (0): MBConvBlock(\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (1): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "                       (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (2): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (3): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (4): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "                       (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (5): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "                       (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (6-7): 2 x MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (8): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "                       (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (9-10): 2 x MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "                       (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (11): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (12-14): 3 x MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "                       (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                   (15): MBConvBlock(\n",
       "                     (_expand_conv): Conv2dStaticSamePadding(\n",
       "                       192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "                       1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "                       (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "                     )\n",
       "                     (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_se_reduce): Conv2dStaticSamePadding(\n",
       "                       1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_se_expand): Conv2dStaticSamePadding(\n",
       "                       48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_project_conv): Conv2dStaticSamePadding(\n",
       "                       1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                       (static_padding): Identity()\n",
       "                     )\n",
       "                     (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                     (_swish): MemoryEfficientSwish()\n",
       "                   )\n",
       "                 )\n",
       "                 (_conv_head): Conv2dStaticSamePadding(\n",
       "                   320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "                   (static_padding): Identity()\n",
       "                 )\n",
       "                 (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "                 (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "                 (_dropout): Dropout(p=0.2, inplace=False)\n",
       "                 (_fc): Sequential(\n",
       "                   (0): Linear(in_features=1280, out_features=2, bias=True)\n",
       "                 )\n",
       "                 (_swish): MemoryEfficientSwish()\n",
       "               ))])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(hiper_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
